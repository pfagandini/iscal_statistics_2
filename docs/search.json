[
  {
    "objectID": "formulae.html",
    "href": "formulae.html",
    "title": "Statistics II ‚Äî Formula Sheet",
    "section": "",
    "text": "ISCAL\n    Lisbon Accounting and Business School\n    Polytechnic University of Lisbon\n  \n  \n    Statistics II ‚Äî Formula Sheet\n    Bachelor's Degree in Management ¬†|¬† Academic Year 2025/2026"
  },
  {
    "objectID": "formulae.html#probability-models",
    "href": "formulae.html#probability-models",
    "title": "Statistics II ‚Äî Formula Sheet",
    "section": "Probability Models",
    "text": "Probability Models\n\nDiscrete Probability Models\n\nBinomial Distribution\n\\[X \\sim \\mathrm{Binomial}(n,\\, p)\\]\n\\[P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}, \\quad x = 0, 1, \\ldots, n\\]\n\\[E(X) = np \\qquad V(X) = npq \\qquad q = 1-p\\]\n\n\n\nHypergeometric Distribution\n\\[X \\sim \\mathrm{Hypergeometric}(N,\\, M,\\, n) \\qquad p = \\dfrac{M}{N}\\]\n\\[P(X = x) = \\dfrac{\\displaystyle\\binom{M}{x}\\binom{N-M}{n-x}}{\\displaystyle\\binom{N}{n}}, \\quad \\max(0,\\, M+n-N) \\leq x \\leq \\min(M,\\, n)\\]\n\\[E(X) = np \\qquad V(X) = npq\\,\\dfrac{N-n}{N-1}\\]\n\n\n\nPoisson Distribution\n\\[X \\sim \\mathrm{Poisson}(\\lambda),\\quad \\lambda &gt; 0\\]\n\\[P(X = x) = \\dfrac{e^{-\\lambda}\\,\\lambda^x}{x!}, \\quad x = 0, 1, 2, \\ldots\\]\n\\[E(X) = V(X) = \\lambda\\]\n\n\n\nGeometric Distribution\n\\[X \\sim \\mathrm{Geometric}(p), \\quad 0 \\leq p \\leq 1\\]\n\\[f(x) = p(1-p)^{x-1}, \\quad x = 1, 2, \\ldots \\qquad F(x) = 1 - (1-p)^k, \\quad k \\leq x &lt; k+1\\]\n\\[E(X) = \\dfrac{1}{p} \\qquad V(X) = \\dfrac{1-p}{p^2}\\]\n\n\n\n\nContinuous Probability Models\n\nUniform Distribution\n\\[X \\sim \\mathrm{Uniform}(a,\\, b), \\quad a,b \\in \\mathbb{R},\\; a &lt; b\\]\n\\[F(x) = \\begin{cases} 0 & x &lt; a \\\\ \\dfrac{x-a}{b-a} & a \\leq x &lt; b \\\\ 1 & x \\geq b \\end{cases}\\]\n\\[E(X) = \\dfrac{a+b}{2} \\qquad V(X) = \\dfrac{(b-a)^2}{12}\\]\n\n\n\nExponential Distribution\n\\[X \\sim \\mathrm{Exponential}(\\lambda), \\quad \\lambda &gt; 0\\]\n\\[F(x) = \\begin{cases} 0 & x \\leq 0 \\\\ 1 - e^{-\\lambda x} & x &gt; 0 \\end{cases}\\]\n\\[E(X) = \\dfrac{1}{\\lambda} \\qquad V(X) = \\dfrac{1}{\\lambda^2}\\]\n\n\n\nNormal Distribution\n\\[X \\sim \\mathrm{Normal}(\\mu,\\, \\sigma), \\quad \\mu \\in \\mathbb{R},\\; \\sigma &gt; 0\\]\n\\[E(X) = \\mu \\qquad V(X) = \\sigma^2\\]\n\\[Z = \\dfrac{X - \\mu}{\\sigma} \\sim \\mathrm{Normal}(0,\\,1) \\qquad E(Z) = 0 \\qquad V(Z) = 1\\]\n\\[\\Phi(z) = P(Z \\leq z) \\qquad \\Phi(-z) = 1 - \\Phi(z)\\]\n\n\n\nt-Student Distribution\n\\[T \\sim t_{(n)}, \\quad n \\text{ degrees of freedom}\\]\n\\[E(T) = 0 \\qquad V(T) = \\dfrac{n}{n-2},\\quad n &gt; 2\\]\n\n\n\nChi-Square Distribution\n\\[Q \\sim \\chi^2_{(n)}, \\quad n \\text{ degrees of freedom}\\]\n\\[E(Q) = n \\qquad V(Q) = 2n\\]\n\\[Y = \\sum_{i=1}^{n} Z_i^2 \\sim \\chi^2_{(n)}, \\quad Z_i \\sim \\mathrm{Normal}(0,\\,1) \\text{ i.i.d.}\\]"
  },
  {
    "objectID": "formulae.html#sampling",
    "href": "formulae.html#sampling",
    "title": "Statistics II ‚Äî Formula Sheet",
    "section": "Sampling",
    "text": "Sampling\n\nSample Statistics\n\\[\\bar{X} = \\dfrac{1}{n}\\sum_{i=1}^{n} X_i\\]\n\\[S^2 = \\dfrac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2 = \\dfrac{1}{n}\\sum X_i^2 - \\bar{X}^2\\]\n\\[S'^{\\,2} = \\dfrac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2 = \\dfrac{1}{n-1}\\sum X_i^2 - \\dfrac{n}{n-1}\\bar{X}^2\\]\n\\[S^2 = \\dfrac{n-1}{n}\\,S'^{\\,2} \\qquad S'^{\\,2} = \\dfrac{n}{n-1}\\,S^2\\]"
  },
  {
    "objectID": "formulae.html#sampling-distributions-and-confidence-intervals-for-mu",
    "href": "formulae.html#sampling-distributions-and-confidence-intervals-for-mu",
    "title": "Statistics II ‚Äî Formula Sheet",
    "section": "Sampling Distributions and Confidence Intervals for \\(\\mu\\)",
    "text": "Sampling Distributions and Confidence Intervals for \\(\\mu\\)\n\n\n\n\n\n\n\n\n\n\n#\nPopulation\nDistribution of \\(\\bar{X}\\)\n\\((1-\\alpha)\\times 100\\%\\) Confidence Interval\n\n\n\n\n1\n\\(X\\sim N(\\mu,\\sigma)\\); \\(\\sigma^2\\) known\n\\(Z = \\dfrac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0,1)\\)\n\\(\\left(\\bar{x} - z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}},\\; \\bar{x} + z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right)\\)\n\n\n2\n\\(X\\sim N(\\mu,\\sigma)\\); \\(\\sigma^2\\) unknown\n\\(T = \\dfrac{\\bar{X}-\\mu}{S'/\\sqrt{n}}\\sim t_{(n-1)}\\)\n\\(\\left(\\bar{x} - t_{\\alpha/2}\\dfrac{s'}{\\sqrt{n}},\\; \\bar{x} + t_{\\alpha/2}\\dfrac{s'}{\\sqrt{n}}\\right)\\)\n\n\n2b\n\\(X\\sim N(\\mu,\\sigma)\\); \\(\\sigma^2\\) unknown, \\(n \\geq 30\\)\n\\(Z = \\dfrac{\\bar{X}-\\mu}{S'/\\sqrt{n}}\\;\\dot{\\sim}\\; N(0,1)\\)\n\\(\\left(\\bar{x} - z_{\\alpha/2}\\dfrac{s'}{\\sqrt{n}},\\; \\bar{x} + z_{\\alpha/2}\\dfrac{s'}{\\sqrt{n}}\\right)\\) (approx.)\n\n\n3a\n\\(X\\sim{?}\\); \\(\\sigma^2\\) known, \\(n \\geq 30\\)\n\\(Z = \\dfrac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\;\\dot{\\sim}\\; N(0,1)\\)\n\\(\\left(\\bar{x} - z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}},\\; \\bar{x} + z_{\\alpha/2}\\dfrac{\\sigma}{\\sqrt{n}}\\right)\\) (approx.)\n\n\n3b\n\\(X\\sim{?}\\); \\(\\sigma^2\\) unknown, \\(n \\geq 30\\)\n\\(Z = \\dfrac{\\bar{X}-\\mu}{S'/\\sqrt{n}}\\;\\dot{\\sim}\\; N(0,1)\\)\n\\(\\left(\\bar{x} - z_{\\alpha/2}\\dfrac{s'}{\\sqrt{n}},\\; \\bar{x} + z_{\\alpha/2}\\dfrac{s'}{\\sqrt{n}}\\right)\\) (approx.)"
  },
  {
    "objectID": "formulae.html#sampling-distribution-and-confidence-interval-for-p",
    "href": "formulae.html#sampling-distribution-and-confidence-interval-for-p",
    "title": "Statistics II ‚Äî Formula Sheet",
    "section": "Sampling Distribution and Confidence Interval for \\(p\\)",
    "text": "Sampling Distribution and Confidence Interval for \\(p\\)\n\n\n\n\n\n\n\n\n\nPopulation\nDistribution of \\(\\hat{p} = X/n\\)\n\\((1-\\alpha)\\times 100\\%\\) Confidence Interval\n\n\n\n\n\\(X\\sim \\mathrm{Binomial}(n,p)\\), \\(n\\geq 30\\)\n\\(\\dfrac{\\hat{p}-p}{\\sqrt{\\hat{p}(1-\\hat{p})/n}}\\;\\dot{\\sim}\\; N(0,1)\\)\n\\(\\left(\\hat{p} - z_{\\alpha/2}\\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}},\\; \\hat{p} + z_{\\alpha/2}\\sqrt{\\dfrac{\\hat{p}(1-\\hat{p})}{n}}\\right)\\) (approx.)"
  },
  {
    "objectID": "formulae.html#sampling-distribution-and-confidence-interval-for-sigma2",
    "href": "formulae.html#sampling-distribution-and-confidence-interval-for-sigma2",
    "title": "Statistics II ‚Äî Formula Sheet",
    "section": "Sampling Distribution and Confidence Interval for \\(\\sigma^2\\)",
    "text": "Sampling Distribution and Confidence Interval for \\(\\sigma^2\\)\n\n\n\n\n\n\n\n\n\nPopulation\nDistribution of \\(S'^{\\,2}\\)\n\\((1-\\alpha)\\times 100\\%\\) Confidence Interval\n\n\n\n\n\\(X\\sim N(\\mu,\\sigma)\\)\n\\(Q = \\dfrac{(n-1)S'^{\\,2}}{\\sigma^2}\\sim\\chi^2_{(n-1)}\\)\n\\(\\left(\\dfrac{(n-1)s'^{\\,2}}{q_{\\sup}},\\; \\dfrac{(n-1)s'^{\\,2}}{q_{\\inf}}\\right)\\)\n\n\n\n\n\\[P(Q &gt; q_{\\inf}) = 1 - \\dfrac{\\alpha}{2} \\qquad P(Q &gt; q_{\\sup}) = \\dfrac{\\alpha}{2}\\]"
  },
  {
    "objectID": "formulae.html#parametric-hypothesis-tests-at-level-alpha",
    "href": "formulae.html#parametric-hypothesis-tests-at-level-alpha",
    "title": "Statistics II ‚Äî Formula Sheet",
    "section": "Parametric Hypothesis Tests (at level \\(\\alpha\\))",
    "text": "Parametric Hypothesis Tests (at level \\(\\alpha\\))\n\n\n\n\n\n\n\n\n\nHypotheses\nTest Statistic (TS)\nCritical Region\n\n\n\n\n\\(H_0: \\theta = \\theta_0 \\;\\text{vs}\\; H_1: \\theta \\neq \\theta_0\\)\n\\(\\mathrm{TS}_0\\)\n\\((-\\infty,\\, q_{\\alpha/2}) \\cup (q_{\\alpha/2},\\, +\\infty)\\)\n\n\n\\(H_0: \\theta \\leq \\theta_0 \\;\\text{vs}\\; H_1: \\theta &gt; \\theta_0\\)\n\\(\\mathrm{TS}_0\\)\n\\((q_{\\alpha},\\, +\\infty)\\)\n\n\n\\(H_0: \\theta \\geq \\theta_0 \\;\\text{vs}\\; H_1: \\theta &lt; \\theta_0\\)\n\\(\\mathrm{TS}_0\\)\n\\((-\\infty,\\, q_{\\alpha})\\)\n\n\n\n\n\\[P(\\mathrm{TS} &gt; q_\\alpha) = \\alpha \\qquad \\text{Reject } H_0 \\text{ if } \\alpha &gt; p\\text{-value}\\]\n\\[\\alpha = P\\!\\left(\\text{Reject } H_0 \\mid H_0 \\text{ true}\\right) \\quad \\text{(Type I error probability)}\\]\n\n\n  ISCAL ¬∑ Instituto Superior de Contabilidade e Administra√ß√£o de Lisboa\n  Statistics II ¬†|¬† 2025/2026"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Statistics II ‚Äî Course Syllabus",
    "section": "",
    "text": "ISCAL\n    Lisbon Accounting and Business School\n    Polytechnic University of Lisbon\n  \n  \n    Bachelor's Degree in Management\n    Program Director: Professor Maria Teresa Fortunato Pereira Martins"
  },
  {
    "objectID": "syllabus.html#course-information",
    "href": "syllabus.html#course-information",
    "title": "Statistics II ‚Äî Course Syllabus",
    "section": "Course Information",
    "text": "Course Information\n\n\n\n\nCourse Unit\nStatistics II\n\n\nArea\nData Science and Statistics\n\n\nCourse Director\nSofia Delgado Ant√≥nio\n\n\nInstructor\nPaulo Fagandini\n\n\nEmail\npfagandini@iscal.ipl.pt\n\n\nLectures\nTuesdays, 11:00 ‚Äì 14:00\n\n\nYear / Semester\n2nd Year / 4th Semester\n\n\nWeekly Contact Hours\n3 Hours\n\n\nECTS\n4"
  },
  {
    "objectID": "syllabus.html#general-objective",
    "href": "syllabus.html#general-objective",
    "title": "Statistics II ‚Äî Course Syllabus",
    "section": "General Objective",
    "text": "General Objective\nThe objective of this course unit is to present several statistical techniques of great practical use in management. Within the context of random phenomena, students will identify and apply probabilistic models and use the notions of statistical inference to make decisions under uncertainty."
  },
  {
    "objectID": "syllabus.html#competencies",
    "href": "syllabus.html#competencies",
    "title": "Statistics II ‚Äî Course Syllabus",
    "section": "Competencies",
    "text": "Competencies\nStudents are expected to acquire and reinforce fundamental concepts of Probability and Statistics. They should become proficient in the main techniques of statistical inference, supported by probability theory, with the primary goal of applying appropriate statistical methods and models to obtain conclusions that support decision-making in business contexts."
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Statistics II ‚Äî Course Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nUpon completion of this course, students should be able to:\n\nInfer characteristics of a population from a sample, applying point and interval estimation techniques;\nUnderstand the general procedures for applying parametric hypothesis tests;\nUnderstand the specification and underlying assumptions of the linear regression model for cross-sectional data;\nDevelop competencies for the joint application of various statistical techniques, using basic software that implements the studied procedures, in order to obtain results that support decision-making under uncertainty."
  },
  {
    "objectID": "syllabus.html#syllabus",
    "href": "syllabus.html#syllabus",
    "title": "Statistics II ‚Äî Course Syllabus",
    "section": "Syllabus",
    "text": "Syllabus\n\n1. Sampling Distributions\n1.1. Central Limit Theorem and applications: Normal approximation to the Binomial and to the Poisson distribution.\n1.2. Concepts of random sample and statistic.\n1.3. Basic results on the sample mean and sample variance.\n1.4. Sampling distributions of the mean in Normal and non-Normal populations. (Includes the t-Student distribution, in the context of this topic.)\n1.5. Sampling distributions of the variance in Normal populations. (Includes the Chi-Square distribution, in the context of this topic.)\n1.6. Sampling distribution of the proportion from a Bernoulli population.\n\n\n\n2. Parameter Estimation\n\n2.1. Point Estimation\n2.1.1. Introduction. Basic notions of estimation: point and interval estimation. The concepts of estimator and estimate.\n2.1.2. Point estimators. Properties of Estimators.\n\n\n2.2. Interval Estimation\n2.2.1. Basic notions.\n2.2.2. Confidence intervals for Normal populations: population mean, population variance, difference between two population means.\n2.2.3. Confidence intervals for non-Normal single-parameter populations (large samples).\n\n\n\n\n3. Parametric Hypothesis Testing\n3.1. Basic concepts: statistical test, null and alternative hypothesis, test statistic, significance level and critical region. Type I and Type II errors.\n3.2. The concept of p-value.\n3.3. Tests involving parameters of Normal populations: tests for means and variances.\n3.4. Tests for the difference between means of two Normal populations.\n3.5. Tests under asymptotic normality conditions (large samples).\n\n\n\n4. The Linear Regression Model\n4.1. Introduction: Theoretical Linear Regression Model, linear relationships.\n4.2. Classical Linear Regression Model for Cross-Sectional Data.\n4.3. OLS Estimation Method. Coefficient estimates and their interpretation.\n4.4. Properties of the OLS estimators of regression coefficients.\n4.5. Goodness of fit: Coefficient of determination and correlation coefficient.\n4.6. Statistical Inference: Hypothesis Tests and Confidence Intervals (one parameter).\n4.7. Validation of the Classical Linear Regression Model Assumptions (Brief Overview)."
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "Statistics II ‚Äî Course Syllabus",
    "section": "Assessment",
    "text": "Assessment\nAssessment may take place under a continuous assessment regime or by final exam.\n\nContinuous Assessment\n\n\n\n\n\n\n\n\n\n\n\nAssessment Element\nWeight\nDuration\nSyllabus Content\nIndicative Date\n\n\n\n\nWritten Test 1\n45%\n1h 20m\nTopics 1 + 2.1\nWeek of 23‚Äì27 March\n\n\nWritten Test 2\n55%\n1h 20m\nTopics 2.2 + 3 + 4\nWeek of 18‚Äì23 May\n\n\n\n\n\nContinuous assessment is based on two (2) written tests, held in person.\nFor approval under this regime, students must obtain a minimum grade of 7 (seven) points in each assessment element. Attendance at all assessment moments is compulsory.\nThe final grade is the weighted average of the two tests:\n\n\\[\\text{Final Grade (CA)} = 0.45\\,(\\text{Test 1}) + 0.55\\,(\\text{Test 2})\\]\n\nStudents under continuous assessment may be subject to an oral examination whenever the instructor deems it necessary to validate the assigned grade.\nNo additional minimum attendance requirements are defined beyond mandatory presence at assessment moments.\n\n\n\nFinal Exam Regime\nStudents who prefer the final exam regime may sit a comprehensive exam worth 100% of the final grade, covering all course content."
  },
  {
    "objectID": "syllabus.html#bibliography",
    "href": "syllabus.html#bibliography",
    "title": "Statistics II ‚Äî Course Syllabus",
    "section": "Bibliography",
    "text": "Bibliography\n\nPrimary Reference (English)\n\nüìñ Newbold, P., Carlson, W. & Thorne, B. (2012) ‚Äî Statistics for Business and Economics. 8th Edition. Prentice Hall.\n\n\n\nAdditional References (Portuguese)\nCust√≥dio, S.G.; Ferreira, T.; Morgado, A.J. & Delgado, S. (2023) ‚Äî An√°lise de Regress√£o Linear. Exerc√≠cios de Aplica√ß√£o Geral e Econ√≥mica. S√≠labas&Desafios.\nFerreira, T. & Cust√≥dio, S.G. (2023) ‚Äî Modelos Probabil√≠sticos. S√≠ntese Te√≥rica e Exerc√≠cios Resolvidos. Edi√ß√µes S√≠labo.\nMorgado, A.J.; Cust√≥dio, S.G. & Ferreira, T. (2023) ‚Äî An√°lise de Regress√£o Linear. Uma abordagem modelar introdut√≥ria. S√≠labas&Desafios.\nMurteira, B.; Silva Ribeiro, C.; Andrade e Silva, J. & Pimenta, C. (2010) ‚Äî Introdu√ß√£o √† Estat√≠stica. Escolar Editora, McGraw-Hill.\nPaulino C. & Branco J. (2012) ‚Äî Exerc√≠cios de Probabilidade e Estat√≠stica. Escolar Editora.\nPedrosa A.C. & Gama S.M.A. (2004) ‚Äî Introdu√ß√£o Computacional √† Probabilidade e Estat√≠stica. Porto Editora.\nPestana, D. D. & Velosa, S. F. (2006) ‚Äî Introdu√ß√£o √† Probabilidade e √† Estat√≠stica. Vol. I. 2¬™ edi√ß√£o. Funda√ß√£o Calouste Gulbenkian.\nPimenta, F., Andrade e Silva, J.; Silva Ribeiro, C. & Murteira, B. (2015) ‚Äî Introdu√ß√£o √† Estat√≠stica ‚Äî 3¬™ Edi√ß√£o. Escolar Editora.\nRobalo A. (2018) ‚Äî Estat√≠stica. Exerc√≠cios. Vol. II. 6¬™ ed.¬†Edi√ß√µes S√≠labo.\nWooldridge, J. M. (2009) ‚Äî Introductory Econometrics. A Modern Approach. 4th Ed. Thomson South-Western.\n\n\n  ISCAL ¬∑ Instituto Superior de Contabilidade e Administra√ß√£o de Lisboa\n  Statistics II ¬∑ 2025/2026"
  },
  {
    "objectID": "exercises/Exercises_SamplingDistributions.html",
    "href": "exercises/Exercises_SamplingDistributions.html",
    "title": "Chapter I - Statistics II - Exercises",
    "section": "",
    "text": "This is a set of exercises for Statistics II, Chapter II ‚Äî Sampling Distributions, from the Lisbon Accounting and Business School (ISCAL‚ÄìIPL).\nContents: t-Student and Chi-Squared distributions. Random sample and statistic. Sampling distributions: of the mean in Normal and non-Normal populations, of the variance in Normal populations, and of the proportion from a Bernoulli population.\nThe exercise set consists of True/False questions (where you should determine the truth value and justify your choice) and multiple choice questions (where you should select the correct option and justify it appropriately)."
  },
  {
    "objectID": "exercises/Exercises_SamplingDistributions.html#questions",
    "href": "exercises/Exercises_SamplingDistributions.html#questions",
    "title": "Chapter I - Statistics II - Exercises",
    "section": "Questions",
    "text": "Questions\n\nThe Concept of a Statistic\n\nQuestion 1\n\nConsider \\((X_1, X_2, \\ldots, X_n)\\), \\(n \\in \\mathbb{N}\\), a random sample from a population \\(X \\sim \\text{Normal}(\\mu = -2,\\; \\sigma)\\), where \\(\\sigma\\) is unknown. Determine whether the following random variables are not statistics:\n\nTRUEFALSE \\(T(X_1, X_2, \\ldots, X_n) = X_1 + X_2 + \\cdots + X_n\\)\nTRUEFALSE \\(T(X_1, X_2, \\ldots, X_n) = \\bar{X}\\)\nTRUEFALSE \\(T(X_1, X_2, \\ldots, X_n) = \\dfrac{\\bar{X}}{\\sigma}\\)\nTRUEFALSE \\(T(X_1, X_2, \\ldots, X_n) = \\dfrac{\\bar{X} - \\mu}{\\sigma / \\sqrt{n}}\\)\nTRUEFALSE \\(T(X_1, X_2, \\ldots, X_n) = \\dfrac{\\sum_{i=1}^{n}(X_i - \\mu)^2}{n}\\)\nTRUEFALSE \\(T(X_1, X_2, \\ldots, X_n) = \\dfrac{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}{n} = S^2\\)\nTRUEFALSE \\(T(X_1, X_2, \\ldots, X_n) = \\dfrac{\\sum_{i=1}^{n}(X_i - \\bar{X})^2}{n-1} = S'^2\\)\n\n\n\nHint\n\nRecall that a statistic is a function of the sample values that does not involve any unknown parameters. Here, \\(\\mu = -2\\) is known but \\(\\sigma\\) is unknown.\n\nItems (a), (b), (f), and (g) depend only on \\(X_i\\) and \\(\\bar{X}\\) (and known constants like \\(n\\)), so they are statistics ‚Äî hence the statement that they are ‚Äúnot statistics‚Äù is False.\nItems (c), (d), and (e) involve the unknown parameter \\(\\sigma\\) or \\(\\mu\\) (note: although \\(\\mu=-2\\) is given, items (d) and (e) include \\(\\sigma\\) as well, and (c) directly uses \\(\\sigma\\)), so they are not statistics ‚Äî the statement is True.\n\nCorrection on (d) and (e): Even though \\(\\mu=-2\\) is known, (d) involves \\(\\sigma\\) (unknown) in the denominator and (e) involves \\(\\mu\\) which, while numerically known here, in items (d) the presence of \\(\\sigma\\) makes it fail. For (e): since \\(\\mu\\) is known, this would actually be a statistic if all parameters involved were known. However, note the formulation carefully: (d) includes \\(\\sigma\\) explicitly ‚Üí not a statistic. (e) includes \\(\\mu\\) (which is known = \\(-2\\)) so actually is a statistic. Let us re-examine:\nActually, since \\(\\mu = -2\\) is known:\n\n\ninvolves \\(\\sigma\\) (unknown) ‚Üí not a statistic ‚Üí True ‚úì\n\n\ninvolves \\(\\sigma\\) (unknown) ‚Üí not a statistic ‚Üí True ‚úì\n\n\ninvolves \\(\\mu\\) (known) but not \\(\\sigma\\) ‚Üí it is a statistic ‚Üí False\n\n\nLet us reconsider (e) with additional context. In the original exercise, (e) is defined as \\(T = \\frac{\\sum(X_i - \\mu)^2}{n}\\), and a separate item (f) from the original list includes \\(\\frac{\\sum\\left(\\frac{X_i - \\mu}{\\sigma}\\right)^2}{}\\). The resolution confirms: (c), (d), and (f-original) are not statistics because they involve \\(\\sigma\\).\n\n\n\n\n\nSampling Distribution of the Mean\n\nQuestion 2 (Normal population, \\(\\sigma\\) unknown ‚Üí t-Student)\n\nSuppose the waiting time on the runway for takeoff at a certain airport is a random variable modelled by a Normal distribution with a mean of 4 minutes. A random sample of 21 aircraft was selected, yielding a corrected sample standard deviation \\(s' = 1.81\\) min.\nTRUEFALSE The probability that the sample mean waiting time exceeds 5 minutes is equal to 0.010.\n\n\nSolution\n\nPopulation: \\(X \\sim \\text{Normal}(\\mu = 4,\\; \\sigma = ?)\\) (standard deviation unknown)\nSample: \\(n = 21 &lt; 30\\); \\(s' = 1.81\\) min.\nSince the population is Normal and \\(\\sigma\\) is unknown, the sampling distribution of the mean is:\n\\[T = \\frac{\\bar{X} - \\mu}{S'/\\sqrt{n}} \\sim t_{(20)}\\]\n\\[P(\\bar{X} &gt; 5) = P\\!\\left(T &gt; \\frac{5 - 4}{1.81/\\sqrt{21}}\\right) = P(T &gt; 2.53) = 0.010\\]\nThe statement is True.\n\n\n\n\nQuestion 3 (Unknown population, \\(\\sigma\\) known ‚Üí CLT)\n\nAt a financial firm, the profit generated by each investment made in the last year has a mean of ‚Ç¨8,200 and a standard deviation of ‚Ç¨3,600. Consider a random sample of 100 investments.\nThe probability that the sample mean differs from the population mean by more than ‚Ç¨806.76 is equal to:\n0.01250.0250.9875None of the above\n\n\nSolution\n\nPopulation: \\(L \\to\\) profit (unknown distribution); \\(\\mu = 8{,}200\\)‚Ç¨; \\(\\sigma = 3{,}600\\)‚Ç¨ (known).\nSample: \\(n = 100 &gt; 30\\).\nBy the CLT: \\[\\bar{X} \\;\\dot{\\sim}\\; N\\!\\left(\\mu,\\; \\frac{\\sigma}{\\sqrt{n}}\\right) \\equiv N(8200,\\; 360)\\]\n\\[P(|\\bar{X} - \\mu| &gt; 806.76) = P\\!\\left(|Z| &gt; \\frac{806.76}{360}\\right) \\approx P(|Z| &gt; 2.241)\\]\n\\[= 2 - 2\\Phi(2.241) = 2 - 2 \\times 0.9875 = 0.025\\]\nThe correct option is b) 0.025.\n\n\n\n\nQuestion 4 (Normal population, \\(\\sigma\\) unknown, \\(n &gt; 30\\) ‚Üí Normal approx.)\n\nA company wishes to validate the value (in euros) of its accounts receivable, which follow a Normal distribution with mean \\(\\mu = 385\\) and unknown standard deviation. A random sample of \\(n\\) independent client accounts yields a corrected sample standard deviation \\(s' = 122.6\\) euros.\nTRUEFALSE The sample size needed (assuming \\(s'\\) remains unchanged) so that the sample mean does not deviate from the population mean by more than 20 euros in 90% of cases is \\(n = 102\\). (Assume, for calculations, that the value of \\(n\\) will be greater than 30.)\n\n\nSolution\n\nPopulation: \\(X \\sim N(\\mu = 385,\\; \\sigma = ?)\\)\nSample: \\(n &gt; 30\\); \\(s' = 122.6\\)\nBy the CLT (since \\(n &gt; 30\\) and \\(\\sigma\\) unknown): \\[\\bar{X} \\;\\dot{\\sim}\\; N\\!\\left(\\mu,\\; \\frac{s'}{\\sqrt{n}}\\right)\\]\nWe want: \\[P(|\\bar{X} - \\mu| &lt; 20) = 0.9 \\iff P\\!\\left(|Z| &lt; \\frac{20}{122.6/\\sqrt{n}}\\right) = 0.9\\] \\[\\iff \\frac{20}{122.6/\\sqrt{n}} = 1.645 \\iff \\sqrt{n} = \\frac{1.645 \\times 122.6}{20}\\] \\[\\iff n \\geq \\left(\\frac{1.645 \\times 122.6}{20}\\right)^2 \\approx 102\\]\nThe statement is True.\n\n\n\n\n\nSampling Distribution of the Proportion\n\nQuestion 5 (Bernoulli population ‚Üí CLT)\n\nSuppose a certain cable TV channel is subscribed to by 10% of the residents of a country. Since this channel has been operating for a very short time, a marketing firm decided to estimate this proportion based on a random sample of 100 residents before renewing its advertising contracts with the channel.\nTRUEFALSE Given that contracts will only be renewed if the sample proportion exceeds 8.5%, the approximate probability of this occurring is 0.6915.\n\n\nSolution\n\nPopulation: \\(X \\sim \\text{Bernoulli}(p = 0.1)\\)\nSample: \\(n = 100 &gt; 30\\) (CLT applies)\n\\[\\hat{p} = \\bar{X} \\;\\dot{\\sim}\\; N\\!\\left(p,\\; \\sqrt{\\frac{pq}{n}}\\right) = N\\!\\left(0.1,\\; \\sqrt{\\frac{0.1 \\times 0.9}{100}}\\right) = N(0.1,\\; 0.03)\\]\n\\[P(\\hat{p} &gt; 0.085) \\approx P\\!\\left(Z &gt; \\frac{0.085 - 0.1}{0.03}\\right) = P(Z &gt; -0.5) = \\Phi(0.5) = 0.6915\\]\nThe statement is True.\n\n\n\n\n\nSampling Distribution of the Variance\n\nQuestion 6 (Normal population, Chi-Squared)\n\nSuppose we wish to analyse the viability of a project whose key element is the price (in monetary units, m.u.) of the raw material, well modelled by a random variable \\(X \\sim N(\\mu,\\; \\sigma)\\).\nA random sample \\((X_1, X_2, \\ldots, X_{16})\\) was selected, yielding a corrected sample variance \\(s'^2 = 0.18\\;\\text{m.u.}^2\\).\n\nTRUEFALSE \\(Q = \\dfrac{15\\,S'^2}{\\sigma^2} \\sim \\chi^2_{(16)}\\)\nTRUEFALSE \\(P(S'^2 &gt; 1.2163\\,\\sigma^2) = 0.25\\)\n\n\n\nSolution\n\nPopulation: \\(X \\sim N(\\mu,\\; \\sigma)\\)\nSample: \\(n = 16\\); \\(s'^2 = 0.18\\)\nThe pivot quantity is: \\[Q = \\frac{(n-1)S'^2}{\\sigma^2} \\sim \\chi^2_{(n-1)}\\]\nSo: \\[Q = \\frac{15\\,S'^2}{\\sigma^2} \\sim \\chi^2_{(\\mathbf{15})}\\]\na) The statement claims \\(Q \\sim \\chi^2_{(16)}\\), but the correct degrees of freedom are \\(n - 1 = 15\\). The statement is False.\nb) \\[P(S'^2 &gt; 1.2163\\,\\sigma^2) = P\\!\\left(\\frac{S'^2}{\\sigma^2} &gt; 1.2163\\right) = P(Q &gt; 15 \\times 1.2163) = P(Q &gt; 18.245)\\]\nReading directly from the \\(\\chi^2\\) table with 15 degrees of freedom: \\(P(Q &gt; 18.245) = 0.25\\).\nThe statement is True.\n\n\n\n\n\nSampling Distribution for the Difference of Means (Paired Samples)\n\nQuestion 7 (Paired samples, small sample, Normal populations)\n\nFifteen adult men, aged between 35 and 50, participated in a study to evaluate the effect of diet and exercise on blood cholesterol levels. Total cholesterol was measured in each individual at the start and after three months of participation in an aerobic exercise programme and a switch to a low-fat diet. The data are shown in the following table:\n\n\n\n\nBlood Cholesterol Level\n\n\n\n\nIndividual\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n\n\n\n\nBefore\n\n\n265\n\n\n240\n\n\n258\n\n\n295\n\n\n251\n\n\n245\n\n\n287\n\n\n314\n\n\n260\n\n\n279\n\n\n283\n\n\n240\n\n\n238\n\n\n225\n\n\n247\n\n\n\n\nAfter\n\n\n229\n\n\n231\n\n\n227\n\n\n240\n\n\n238\n\n\n241\n\n\n234\n\n\n256\n\n\n247\n\n\n239\n\n\n246\n\n\n218\n\n\n219\n\n\n226\n\n\n233\n\n\n\n\nAssume that:\n\nThe samples (Before and After) are paired (each column corresponds to the same individual).\nEach pair of observations is independent.\nCholesterol levels follow a Normal distribution.\n\nTRUEFALSE The probability that the mean of the difference between Before and After differs from its expected value by more than 6.61 is equal to 0.2.\n\n\nSolution\n\nLet \\(X_1\\) be the cholesterol before treatment and \\(X_2\\) the cholesterol after treatment.\nDefine \\(D = X_1 - X_2\\). Since both \\(X_1\\) and \\(X_2\\) are Normal, so is \\(D\\).\nComputing the differences:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nDiff\n36\n9\n31\n55\n13\n4\n53\n58\n13\n40\n37\n22\n19\n‚àí1\n14\n\n\n\nFrom the data: \\(\\bar{d} = 26.87\\); \\(s'_D = 19.04\\).\nPopulation: Normal, \\(\\sigma\\) unknown; Sample: \\(n = 15 &lt; 30\\) ‚Üí t-Student.\n\\[T = \\frac{\\bar{D} - \\mu_D}{S'_D / \\sqrt{n}} \\sim t_{(n-1)} \\equiv t_{(14)}\\]\n\\[P(|\\bar{D} - \\mu_D| &gt; 6.61) = P\\!\\left(|T| &gt; \\frac{6.61}{19.04/\\sqrt{15}}\\right) = P\\!\\left(|T| &gt; \\frac{6.61 \\times \\sqrt{15}}{19.04}\\right)\\]\n\\[= P(|T| &gt; 1.345) = 2\\,P(T &gt; 1.345)\\]\nReading from the t-distribution table with 14 degrees of freedom: \\(P(T &gt; 1.345) = 0.1\\).\n\\[= 2 \\times 0.1 = 0.2\\]\nThe statement is True.\n\n\n\nExercises adapted from the exercise booklet by Prof.¬†Teresa Ferreira and Prof.¬†Sandra Cust√≥dio, ISCAL‚ÄìIPL. Solutions based on ‚ÄúUma Resolu√ß√£o Poss√≠vel‚Äù (2024/2025)."
  },
  {
    "objectID": "slides/01_CLT_slides.html#additivity-normal-distribution",
    "href": "slides/01_CLT_slides.html#additivity-normal-distribution",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Additivity (Normal Distribution):\nIf:\n\n\\(X_i \\sim Normal(\\mu,\\sigma)\\), \\(i = 1,\\ldots,n\\) ‚Äî independent and identically distributed random variables\n\\(T = X_1 + \\ldots + X_n\\) and \\(\\bar{X} = \\dfrac{T}{n}\\)\n\nthen:\n\\[T \\sim Normal\\!\\left(\\mu_T = n \\times \\mu,\\sigma_T = \\sqrt{n} \\times \\sigma\\right)\\]\n\\[\\bar{X} \\sim Normal\\!\\left(\\mu_{\\bar{X}} = \\mu,\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\\right)\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-1",
    "href": "slides/01_CLT_slides.html#example-1",
    "title": "Central Limit Theorem",
    "section": "Example 1",
    "text": "Example 1\nConsider:\n\nIndependent variables: \\(X_i \\sim Normal(\\mu_i = 40,\\sigma_i = 10)\\)\n\\(X_i\\) ‚Äî lunch duration of student \\(i\\) (in minutes)\n\\(\\bar{X} = \\dfrac{1}{n}(X_1 + \\ldots + X_n)\\) ‚Äî sample mean (\\(n = 25\\) and \\(n = 81\\) students)\n\nFind the probability that the sample mean lies between 39 and 41 minutes.\nSolution: (assuming Normal distribution)\n\\[\\bar{X} \\sim Normal\\!\\left(\\mu_{\\bar{X}} = 40,\\sigma_{\\bar{X}} = \\frac{10}{\\sqrt{n}}\\right)\\]\n\\[\\Rightarrow P\\!\\left(39 &lt; \\bar{X} \\leq 41\\right) = P\\!\\left(\\frac{39-40}{\\tfrac{10}{\\sqrt{n}}} &lt; Z \\leq \\frac{41-40}{\\tfrac{10}{\\sqrt{n}}}\\right)\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-1-solution-cont.",
    "href": "slides/01_CLT_slides.html#example-1-solution-cont.",
    "title": "Central Limit Theorem",
    "section": "Example 1 ‚Äî Solution (cont.)",
    "text": "Example 1 ‚Äî Solution (cont.)\n\\(P(39 &lt; \\bar{X} \\leq 41) =\\)\n\n\\(n = 25\\): \\(= P(-0.5 &lt; Z \\leq 0.5) = \\Phi(0.5) - \\Phi(-0.5) = \\mathbf{0.3829}\\)\n\\(n = 81\\): \\(= P(-0.9 &lt; Z \\leq 0.9) = \\Phi(0.9) - \\Phi(-0.9) = \\mathbf{0.6319}\\)\n\n\n\n\n\n\n\n\n\n\n\nComment: As sample size increases, the probability that the sample observations concentrate around the mean also increases."
  },
  {
    "objectID": "slides/01_CLT_slides.html#central-limit-theorem-clt",
    "href": "slides/01_CLT_slides.html#central-limit-theorem-clt",
    "title": "Central Limit Theorem",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\nCentral Limit Theorem (CLT):\nIf:\n\n\\(X_i \\sim \\textit{Distribution}(\\mu,\\sigma)\\), \\(i = 1,\\ldots,n\\) ‚Äî independent and identically distributed random variables\n\\(T = X_1 + \\ldots + X_n\\), \\(\\quad \\bar{X} = \\dfrac{T}{n}\\)\n\nthen, for \\(n\\) sufficiently large (as a rule, \\(n \\geq 30\\)):\n\n\\[T \\;\\dot{\\sim}\\; Normal\\!\\left(\\mu_T = n \\times \\mu,\\sigma_T = \\sqrt{n} \\times \\sigma\\right)\\]\n\\[\\bar{X} \\;\\dot{\\sim}\\; Normal\\!\\left(\\mu_{\\bar{X}} = \\mu,\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\\right)\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-1-revisited-clt",
    "href": "slides/01_CLT_slides.html#example-1-revisited-clt",
    "title": "Central Limit Theorem",
    "section": "Example 1 ‚Äî Revisited (CLT)",
    "text": "Example 1 ‚Äî Revisited (CLT)\nConsider:\n\nIndependent variables: \\(X_i \\sim \\textit{Distribution}(\\mu_i = 40,\\sigma_i = 10)\\)\n\\(X_i\\) ‚Äî lunch duration of student \\(i\\) (in minutes)\n\\(\\bar{X} = \\dfrac{1}{n}(X_1 + \\ldots + X_n)\\) ‚Äî sample mean\n\\(n = 25,\\; n = 81,\\; n = 225\\) students\n\nFind \\(P(39 &lt; \\bar{X} \\leq 41)\\). Solution: (without the assumption of normality)\nWith \\(\\mathbf{n \\geq 30}\\):\n\\[\\bar{X} \\;\\dot{\\sim}\\; Normal\\!\\left(\\mu_{\\bar{X}} = 40,\\sigma_{\\bar{X}} = \\frac{10}{\\sqrt{n}}\\right) \\quad \\xrightarrow{CLT}\\]\n\\[\\Rightarrow P(39 &lt; \\bar{X} \\leq 41) \\approx P\\!\\left(\\frac{39-40}{\\tfrac{10}{\\sqrt{n}}} &lt; Z \\leq \\frac{41-40}{\\tfrac{10}{\\sqrt{n}}}\\right) =\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-1-revisited-cont.",
    "href": "slides/01_CLT_slides.html#example-1-revisited-cont.",
    "title": "Central Limit Theorem",
    "section": "Example 1 ‚Äî Revisited (cont.)",
    "text": "Example 1 ‚Äî Revisited (cont.)\n\\(P(39 &lt; \\bar{X} \\leq 41) \\rightarrow (CLT)\\)\n\n\\(n = 25\\): insufficient information for reliable calculations (\\(n &lt; 30\\))\n\\(n = 81\\): \\(P(-0.9 &lt; Z \\leq 0.9) = \\Phi(0.9) - \\Phi(-0.9) = \\mathbf{0.6319}\\)\n\\(n = 225\\): \\(P(-1.5 &lt; Z \\leq 1.5) = \\Phi(1.5) - \\Phi(-1.5) = \\mathbf{0.8664}\\)\n\n\n\n\n\n\n\n\n\n\n\nComment: The probability that observations concentrate around the mean increased with the larger sample (\\(n = 225\\))."
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-2",
    "href": "slides/01_CLT_slides.html#example-2",
    "title": "Central Limit Theorem",
    "section": "Example 2",
    "text": "Example 2\nConsider the independent variables:\n\\(X_i\\) ‚Äî number of points on the \\(i\\)-th roll of a die :game_die:\nFind: \\(P(2.5 &lt; \\bar{X} &lt; 3.5)\\) for \\(n = 1,\\; n = 2,\\; n = 36\\) rolls.\nSolution:\n\\(n = 1\\): \\(= P(X_1 = 3) = \\dfrac{1}{6}\\)\n\\(n = 2\\):\n\\(= P(X_1=1,\\,X_2=5) + P(X_1=2,\\,X_2=4) + P(X_1=3,\\,X_2=3)\\) \\(+ P(X_1=4,\\,X_2=2) + \\ldots + P(X_1=5,\\,X_2=1) = \\dfrac{5}{36}\\)"
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-2-solution-cont.",
    "href": "slides/01_CLT_slides.html#example-2-solution-cont.",
    "title": "Central Limit Theorem",
    "section": "Example 2 ‚Äî Solution (cont.)",
    "text": "Example 2 ‚Äî Solution (cont.)\n\\(n = 36\\) (CLT): note that for a fair die \\(\\mu = 3.5\\) and \\(\\sigma = 1.7078\\)\n\\[= P\\!\\left(\\frac{2.5 - 3.5}{\\tfrac{1.7078}{\\sqrt{36}}} &lt; Z &lt; \\frac{3.5 - 3.5}{\\tfrac{1.7078}{\\sqrt{36}}}\\right) = \\Phi(0) - \\Phi(-3.51) = 0.4998\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-2-solution-cont.-1",
    "href": "slides/01_CLT_slides.html#example-2-solution-cont.-1",
    "title": "Central Limit Theorem",
    "section": "Example 2 ‚Äî Solution (cont.)",
    "text": "Example 2 ‚Äî Solution (cont.)\n\n\n\n\n\n\n\n\n\nComments:\n\nThe plots illustrate convergence towards the Normal distribution.\nThe variable is discrete ‚Äî see the continuity correction ahead."
  },
  {
    "objectID": "slides/01_CLT_slides.html#corollaries-of-the-clt",
    "href": "slides/01_CLT_slides.html#corollaries-of-the-clt",
    "title": "Central Limit Theorem",
    "section": "Corollaries of the CLT",
    "text": "Corollaries of the CLT\n\nCorollary 1\nLet \\(X\\) be a random variable with a Binomial distribution with parameters \\(n\\) and \\(p\\), \\(X \\sim binomial(n,p)\\). If \\(n \\geq 30\\), \\(np &gt; 5\\) or \\(n(1-p) &gt; 5\\), then:\n\\[X \\;\\underset{CLT}{\\dot{\\sim}}\\; Normal\\!\\left(\\mu = np,\\;\\sigma = \\sqrt{npq}\\right) \\;\\Leftrightarrow\\; Z_n = \\frac{X - np}{\\sqrt{npq}} \\;\\underset{CLT}{\\dot{\\sim}}\\; Normal(0,1)\\]\n\n\nCorollary 2\nLet \\(X\\) be a random variable with a Poisson distribution with parameter \\(\\lambda\\), \\(X \\sim Poisson(\\lambda)\\). In practice, if \\(\\lambda &gt; 20\\), then:\n\\[X \\;\\underset{CLT}{\\dot{\\sim}}\\; Normal\\!\\left(\\mu = \\lambda,\\;\\sigma = \\sqrt{\\lambda}\\right) \\;\\Leftrightarrow\\; Z_n = \\frac{X - \\lambda}{\\sqrt{\\lambda}} \\;\\underset{CLT}{\\dot{\\sim}}\\; Normal(0,1)\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#continuity-correction",
    "href": "slides/01_CLT_slides.html#continuity-correction",
    "title": "Central Limit Theorem",
    "section": "Continuity Correction",
    "text": "Continuity Correction\n\n\n\n\n\n\nNote\n\n\n\nNote: In both corollaries, we are approximating the distribution of a discrete random variable with a continuous (Normal) distribution. The continuity correction should therefore be applied:\n\\[[F(x)]_{\\text{Discrete}} \\approx [F(x + 0.5)]_{\\text{Normal}}\\]\n\n\nExample: \\(X \\sim Poisson(\\lambda = 30)\\)\n\nExact probability: \\(P(X = 40) = 0.01394\\)\nWithout correction: \\(P\\!\\left(Z = \\dfrac{40-30}{\\sqrt{30}}\\right) = 0\\) ‚Äî unusable!\nWith continuity correction:\n\n\\[P(X = 40) = P(39.5 \\leq X \\leq 40.5) \\approx P\\!\\left(\\frac{39.5-30}{\\sqrt{30}} \\leq Z \\leq \\frac{40.5-30}{\\sqrt{30}}\\right)\\]\n\\[= \\Phi(1.92) - \\Phi(1.73) = 0.9726 - 0.9582 = \\mathbf{0.01444}\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#continuity-correction-other-cases",
    "href": "slides/01_CLT_slides.html#continuity-correction-other-cases",
    "title": "Central Limit Theorem",
    "section": "Continuity Correction ‚Äî Other Cases",
    "text": "Continuity Correction ‚Äî Other Cases\nRemaining cases (discrete \\(X\\), applying continuity correction):\n\\[P(X &lt; 40) = P(X \\leq 39) = P(X \\leq 39.5) \\approx \\cdots\\]\n\\[P(X &gt; 40) = P(X \\geq 41) = P(X \\geq 40.5) \\approx \\cdots\\]\n\\[P(35 &lt; X &lt; 40) = P(36 \\leq X \\leq 39) = P(35.5 \\leq X \\leq 39.5) \\approx \\cdots\\]\n\\[P(35 \\leq X &lt; 40) = P(35 \\leq X \\leq 39) = P(34.5 \\leq X \\leq 39.5) \\approx \\cdots\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-3",
    "href": "slides/01_CLT_slides.html#example-3",
    "title": "Central Limit Theorem",
    "section": "Example 3",
    "text": "Example 3\nConsider:\n\nIndependent variables: \\(X_i \\sim binomial(n_i = 1\\;;\\;p = 0.05)\\)\n\\(X_i = 1\\) if a tax return contains errors (otherwise \\(X_i = 0\\))\nConsider an inspection of a sample of \\(n = 1000\\) tax returns.\n\nFind the probability that there are at least 60 incorrect returns.\nSolution: (exact and approximate distributions)\nExact distribution:\n\\(T \\sim binomial(n = 1000\\;;\\;p = 0.05)\\) ‚Äî by the additivity of the Binomial\n\\[P(T \\geq 60) = 1 - P(T &lt; 60) = 1 - P(T \\leq 59) = \\mathbf{0.0867}\\]"
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-3-approximate-solution",
    "href": "slides/01_CLT_slides.html#example-3-approximate-solution",
    "title": "Central Limit Theorem",
    "section": "Example 3 ‚Äî Approximate Solution",
    "text": "Example 3 ‚Äî Approximate Solution\nApproximate distribution (exact and approximate):\n\\[\\mu = n_i p = 1 \\times 0.05 \\qquad \\sigma = \\sqrt{n_i p(1-p)} = \\sqrt{1 \\times 0.05 \\times 0.95} = \\sqrt{0.0475}\\]\n\\[T \\;\\dot{\\sim}\\; Normal\\!\\left(\\mu_T = 1000 \\times 0.05\\;;\\;\\sigma_T = \\sqrt{1000} \\times \\sqrt{0.0475}\\right)\\]\nApplying the continuity correction:\n\\[P(T \\geq 60) \\;\\underset{\\text{cont. corr.}}{=}\\; P(T \\geq 59.5) = 1 - P(T &lt; 59.5)\\]\n\\[\\underset{CLT}{\\approx} 1 - P\\!\\left(Z \\leq \\frac{59.5 - 50}{6.89}\\right) = 1 - \\Phi(1.38) = \\mathbf{0.0838}\\]\n\nComment: The variable is discrete, so we applied a continuity correction."
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-4-exercise-1",
    "href": "slides/01_CLT_slides.html#example-4-exercise-1",
    "title": "Central Limit Theorem",
    "section": "Example 4 [Exercise 1]",
    "text": "Example 4 [Exercise 1]\nConsider: \\(T \\sim Poisson(\\lambda_T = 30)\\)\n\\(T\\) = number of daily accesses to a website\nFind \\(P(T &gt; 40)\\).\nSolution: (exact and approximate distributions)\nExact distribution:\n\\[P(T &gt; 40) = 1 - P(T \\leq 40) = 0.0323 \\simeq 0.03\\]\n\nNote: Since this is a homogeneous Poisson process, the day can be viewed as a sum of \\(n\\) sub-periods, each with the same probability distribution."
  },
  {
    "objectID": "slides/01_CLT_slides.html#example-4-approximate-solution",
    "href": "slides/01_CLT_slides.html#example-4-approximate-solution",
    "title": "Central Limit Theorem",
    "section": "Example 4 ‚Äî Approximate Solution",
    "text": "Example 4 ‚Äî Approximate Solution\nApproximate distribution:\n\\(T = X_1 + \\ldots + X_n\\) (by additivity), where \\(X_i \\sim Poisson\\!\\left(\\lambda = \\dfrac{30}{n}\\right)\\)\n\\[T \\sim Poisson(\\lambda = 30) \\;\\Rightarrow\\; T \\;\\underset{CLT}{\\dot{\\sim}}\\; Normal\\!\\left(\\mu = \\lambda = 30,\\;\\sigma = \\sqrt{\\lambda} = \\sqrt{30}\\right)\\]\nApplying the continuity correction:\n\\[P(T &gt; 40) \\;\\underset{\\text{cont. corr.}}{=}\\; P(T \\geq 40.5) \\;\\underset{CLT}{\\approx}\\; P\\!\\left(Z \\geq \\frac{40.5 - 30}{\\sqrt{30}}\\right) = P(Z \\geq 1.92)\\]\n\\[= 1 - \\Phi(1.92) = 1 - 0.9726 = \\mathbf{0.0274}\\]\nApproximation error: \\(\\varepsilon = |0.0323 - 0.0274| = 0.0049\\)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "iscal_statistics_2",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#what-well-cover",
    "href": "slides/02_sampling_distributions.html#what-well-cover",
    "title": "Sampling Distributions",
    "section": "What We‚Äôll Cover",
    "text": "What We‚Äôll Cover\nTopics:\n\nRandom Samples and Statistics\nSampling Distributions of the Sample Mean\n\nNormal populations (known and unknown \\(\\sigma\\))\nNon-normal populations\n\nSampling Distribution of the Sample Variance\nSampling Distribution of Sample Proportion\nt-Student and Chi-square Distributions"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#important-concepts",
    "href": "slides/02_sampling_distributions.html#important-concepts",
    "title": "Sampling Distributions",
    "section": "Important Concepts",
    "text": "Important Concepts\n\n\n\nRandom Sample\n\n\nA sample of size \\(n\\), drawn from a population according to a random process, is called a random sample if the values of the random variables \\(X_1, X_2, \\ldots, X_n\\) are independent and identically distributed.\n\n\\(X_1\\) denotes the first element of the sample\n\\(X_2\\) denotes the second element\n\\(X_n\\) denotes the \\(n\\)-th element"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#important-concepts-1",
    "href": "slides/02_sampling_distributions.html#important-concepts-1",
    "title": "Sampling Distributions",
    "section": "Important Concepts",
    "text": "Important Concepts\n\n\n\nStatistical Inference\n\n\nStatistical inference is the use of a set of methods that allows to draw conclusions about a population, from a random sample of that population."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#parameters-and-statistics",
    "href": "slides/02_sampling_distributions.html#parameters-and-statistics",
    "title": "Sampling Distributions",
    "section": "Parameters and Statistics",
    "text": "Parameters and Statistics\n\n\n\nParameter\n\n\nA parameter is a value that characterizes a given population which, although unknown, is fixed.\nExamples: population mean \\(\\mu\\), population variance \\(\\sigma^2\\)\n\n\n\n\n\n\n\nStatistic\n\n\nA statistic is a characteristic that is a function of the sample values; that is, it‚Äôs a random variable that does not involve any unknown parameters. If the statistic is used to estimate a parameter, then this statistic is called an estimator.\nSymbolized by: \\(T(X_1, X_2, \\ldots, X_n)\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#parameters-and-statistics-1",
    "href": "slides/02_sampling_distributions.html#parameters-and-statistics-1",
    "title": "Sampling Distributions",
    "section": "Parameters and Statistics",
    "text": "Parameters and Statistics\n\n\n\nEstimate\n\n\nAn estimate for a parameter is the realized value of a statistic, i.e., the value you obtain from aplying the function statistic to a particular sample.\n\n\n\nGiven that the sample is random, because it is a collection of realized random variables, the statistic is itself a random variable. Every time you draw a new sample, you obtain a different estimate."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#example-what-is-a-statistic",
    "href": "slides/02_sampling_distributions.html#example-what-is-a-statistic",
    "title": "Sampling Distributions",
    "section": "Example: What is a Statistic?",
    "text": "Example: What is a Statistic?\nConsider a random sample \\((X_1, X_2, \\ldots, X_n)\\), \\(n \\in \\mathbb{N}\\), from a population \\(X \\sim Normal(\\mu = -2, \\sigma^2)\\), where \\(\\sigma^2\\) is unknown.\nWhich of the following are statistics?\n\n\\(T(X_1, X_2, \\ldots, X_n) = X_1 + X_2 + \\cdots + X_n\\)\n\\(T(X_1, X_2, \\ldots, X_n) = \\frac{X_1 + X_2 + \\cdots + X_n}{n} = \\bar{X}\\)\n\\(T(X_1, X_2, \\ldots, X_n) = \\frac{\\bar{X}}{\\sigma}\\)\n\\(T(X_1, X_2, \\ldots, X_n) = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}}\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#example-solution",
    "href": "slides/02_sampling_distributions.html#example-solution",
    "title": "Sampling Distributions",
    "section": "Example: Solution",
    "text": "Example: Solution\n\na) YES - Sum of sample values (no unknown parameters)\nb) YES - Sample mean (no unknown parameters)\nc) NO - Contains \\(\\sigma\\) which is unknown\nd) NO - Contains both \\(\\mu\\) (known = -2, so this part is OK) and \\(\\sigma\\) (unknown)\n\n\nNote: For it to be a statistic, we must be able to calculate it from the sample data alone, without knowledge of unknown population parameters."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#example-2",
    "href": "slides/02_sampling_distributions.html#example-2",
    "title": "Sampling Distributions",
    "section": "Example 2",
    "text": "Example 2\n\nLet \\((X_1,X_2,\\dots,X_n),n\\in\\mathbb{N}\\), a random sample from a population \\(X\\) (with unknown distribution), but with expected value \\(\\mu\\) and variance \\(\\sigma^2\\), both known:\n\n\nThe expected value of \\(\\sum_{i=1}^n X_i\\) and \\(\\bar{X}\\) are:\n\n\\(n\\mu\\) and \\(\\mu\\)\n\\(n\\mu\\) and \\(\\mu/n\\)\n\\(\\mu\\) and \\(\\mu/n\\)\nNone of the above\n\n\nThe variance of \\(\\sum_{i=1}^n X_i\\) and \\(\\bar{X}\\) are:\n\n\\(n\\sigma^2\\) and \\(\\sigma^2\\)\n\\(n\\sigma^2\\) and \\(\\sigma^2/n\\)\n\\(\\sigma^2\\) and \\(\\sigma^2/n\\)\nNone of the above\n\n\n\n\nAnswer: (a) and (b)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#the-sample-average-as-an-estimator-for-mu",
    "href": "slides/02_sampling_distributions.html#the-sample-average-as-an-estimator-for-mu",
    "title": "Sampling Distributions",
    "section": "The sample average as an estimator for \\(\\mu\\)",
    "text": "The sample average as an estimator for \\(\\mu\\)\nSuppose you have a random sample \\(\\{X_i\\}_{i=1}^n\\). Then we just saw the average is going to be:\n\\[\\bar{X}=\\frac{\\sum_{i=1}^n X_i}{n}\\]\nAnd we saw that \\(E[\\bar{X}]=\\mu\\), and \\(V[\\bar{X}]=\\frac{\\sigma^2}{n}\\). This second result is beautiful. Note that we did not make any assumptions on the distribution of \\(X\\); if we gather a large sample, its average will be close to the true mean with higher probability."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#a-question",
    "href": "slides/02_sampling_distributions.html#a-question",
    "title": "Sampling Distributions",
    "section": "A question",
    "text": "A question\n\nHow could we obtain the distribution, using a random sample, for a statistic or a random variable?\n\n\nTo answer this, we need to introduce two new distributions (not covered in Statistics I)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#chi2-distribution-1",
    "href": "slides/02_sampling_distributions.html#chi2-distribution-1",
    "title": "Sampling Distributions",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution\nLet \\(Z_1,Z_2,\\dots, Z_n\\) random variables iid distributed \\(Z_i\\sim N(0,1)\\).\n\\[Y=\\sum_{i=1}^n Z_i^2=\\sum_{i=1}^n\\left(\\frac{X_i-\\mu}{\\sigma}\\right)^2\\]\nis a random variable distributed \\(\\chi^2_{\\nu}\\), where \\(\\nu\\) represents the degrees of freedom."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#degrees-of-freedom",
    "href": "slides/02_sampling_distributions.html#degrees-of-freedom",
    "title": "Sampling Distributions",
    "section": "Degrees of Freedom",
    "text": "Degrees of Freedom\n\n\n\nDefinition\n\n\nThe number of independent values that are free to vary in your data without breaking any constraints.\n\n\n\nFormula: \\(dof = N - P\\)\n\n\\(N\\) = sample size\n\n\\(P\\) = number of parameters estimated"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#the-intuitive-example",
    "href": "slides/02_sampling_distributions.html#the-intuitive-example",
    "title": "Sampling Distributions",
    "section": "The Intuitive Example",
    "text": "The Intuitive Example\nImagine you have 4 numbers that must average to 20.\n\nFirst person picks: 15 ‚úì (free to choose)\nSecond person picks: 30 ‚úì (free to choose)\n\nThird person picks: 10 ‚úì (free to choose)\nFourth person picks: ? ‚úó (not free!)\n\nThe fourth number must be 25 to keep the average at 20.\nResult: 3 degrees of freedom (\\(df = 4 - 1 = 3\\))"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#why-degrees-of-freedom-matter",
    "href": "slides/02_sampling_distributions.html#why-degrees-of-freedom-matter",
    "title": "Sampling Distributions",
    "section": "Why Degrees of Freedom Matter",
    "text": "Why Degrees of Freedom Matter\nIn Practice:\n\n‚úì More \\(df\\) ‚Üí More reliable estimates\n\n‚úì Affects the shape of distributions (\\(t\\), \\(\\chi^2\\), \\(F\\))\n\nKey Insight:\nWhen you estimate a parameter (like the mean), you ‚Äúspend‚Äù one degree of freedom. It‚Äôs the cost of using your data to calculate statistics."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution",
    "href": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution",
    "title": "Sampling Distributions",
    "section": "Back to the \\(\\chi^2\\) Distribution",
    "text": "Back to the \\(\\chi^2\\) Distribution\n\\(X\\sim N(0,1)\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution-1",
    "href": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution-1",
    "title": "Sampling Distributions",
    "section": "Back to the \\(\\chi^2\\) Distribution",
    "text": "Back to the \\(\\chi^2\\) Distribution\nHow is \\(Y=X^2\\) distributed?\n\nThis is the \\(\\chi^2_1\\)!"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution-2",
    "href": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution-2",
    "title": "Sampling Distributions",
    "section": "Back to the \\(\\chi^2\\) Distribution",
    "text": "Back to the \\(\\chi^2\\) Distribution\nHow is \\(Y_1+Y_2=X_1^2+X_2^2\\) distributed?\n\nThis is the \\(\\chi^2_2\\)!"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution-3",
    "href": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution-3",
    "title": "Sampling Distributions",
    "section": "Back to the \\(\\chi^2\\) Distribution",
    "text": "Back to the \\(\\chi^2\\) Distribution\nHow is \\(\\sum_{i=1}^{10} Y_i\\) distributed?\n\nThis is the \\(\\chi^2_{10}\\)!"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution-4",
    "href": "slides/02_sampling_distributions.html#back-to-the-chi2-distribution-4",
    "title": "Sampling Distributions",
    "section": "Back to the \\(\\chi^2\\) Distribution",
    "text": "Back to the \\(\\chi^2\\) Distribution\nHow is \\(\\sum_{i=1}^{100} Y_i\\) distributed?\n\nThis is the \\(\\chi^2_{100}\\)!"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#properties-of-chi2",
    "href": "slides/02_sampling_distributions.html#properties-of-chi2",
    "title": "Sampling Distributions",
    "section": "Properties of \\(\\chi^2\\)",
    "text": "Properties of \\(\\chi^2\\)\nKey Properties:\n\n\\(E[Y] = \\nu\\) (mean equals degrees of freedom)\n\\(V[Y] = 2\\nu\\)\nRight-skewed distribution (not symmetric)\nOnly positive values\nSum property: If \\(Q_1 \\sim \\chi^2_{\\nu_1}\\) and \\(Q_2 \\sim \\chi^2_{\\nu_2}\\) are independent, then \\(Q_1 + Q_2 \\sim \\chi^2_{\\nu_1 + \\nu_2}\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#chi2-distribution-shape",
    "href": "slides/02_sampling_distributions.html#chi2-distribution-shape",
    "title": "Sampling Distributions",
    "section": "\\(\\chi^2\\) Distribution Shape",
    "text": "\\(\\chi^2\\) Distribution Shape\n\n\\(\\chi^2\\) distributions for different degrees of freedom. As \\(\\nu\\) increases, the distribution becomes more symmetric."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#chi2-distribution-shape-1",
    "href": "slides/02_sampling_distributions.html#chi2-distribution-shape-1",
    "title": "Sampling Distributions",
    "section": "\\(\\chi^2\\) Distribution Shape",
    "text": "\\(\\chi^2\\) Distribution Shape\n\\[\\nu\\rightarrow\\infty\\ \\Rightarrow\\ \\chi^2_{\\nu}\\rightarrow N(\\nu,2\\nu)\\]\nThanks to the Central Limit Theorem (forthcoming). Note that this equivalent to say that,\n\\[\\nu\\rightarrow\\infty\\ \\Rightarrow\\ Z_n=\\frac{\\chi^2_{\\nu}-\\nu}{\\sqrt{2\\nu}}\\sim N(0,1)\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#using-chi2-tables",
    "href": "slides/02_sampling_distributions.html#using-chi2-tables",
    "title": "Sampling Distributions",
    "section": "Using \\(\\chi^2\\) Tables",
    "text": "Using \\(\\chi^2\\) Tables\nFor a \\(\\chi^2\\) distribution with \\(\\nu\\) degrees of freedom:\n\nTables provide: \\(P(Y &gt; \\chi^2_{\\alpha, \\nu}) = \\alpha\\)\nImportant: Not symmetric, so \\(\\chi^2_{1-\\alpha, \\nu} \\neq -\\chi^2_{\\alpha, \\nu}\\)\n\n\nExample: For \\(\\nu = 10\\):\n\n\\(\\chi^2_{0.05, 10} = 18.307\\) means \\(P(Y &gt; 18.307) = 0.05\\)\n\\(\\chi^2_{0.95, 10} = 3.940\\) means \\(P(Y &gt; 3.940) = 0.95\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise",
    "href": "slides/02_sampling_distributions.html#exercise",
    "title": "Sampling Distributions",
    "section": "Exercise",
    "text": "Exercise\nLet \\(\\{Z_i\\}_{i=1}^5\\) a random sample from a population \\(N(0,1)\\) and \\(Y=\\sum_{i=1}^5Z_i^2\\)\n\nT/F \\(P(1.610&lt;Y&lt;9.236)=0.8\\)\nT/F The standard deviation of \\(Z_3^2+Z_4^2\\) is equal to 4."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-1",
    "href": "slides/02_sampling_distributions.html#exercise-1",
    "title": "Sampling Distributions",
    "section": "Exercise",
    "text": "Exercise\n\n\\(Y\\sim \\chi^2_5\\) and then \\[\n\\begin{aligned}\n   P(1.61&lt;Y&lt;9.236)&=P(Y&gt;1.61)-P(Y&gt;9.236)\\\\&=0.9-0.1=0.8\n\\end{aligned}\n\\]\n\\(Z_3^2\\sim \\chi_1^2\\), \\(Z_4^2\\sim \\chi_1^2\\), then \\(Z_3^2+Z_4^2\\sim \\chi_2^2\\): \\[V[Z_3^2+Z_4^2]=2\\times 2 = 4\\ \\Rightarrow\\ \\sigma = 2\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#the-t-student-distribution",
    "href": "slides/02_sampling_distributions.html#the-t-student-distribution",
    "title": "Sampling Distributions",
    "section": "The t-Student Distribution",
    "text": "The t-Student Distribution\nLet \\(X\\sim N(0,1)\\) and \\(Y\\sim \\chi^2_\\nu\\), independent random variables. Then, the random variable \\[T=\\frac{X}{\\sqrt{\\frac{Y}{\\nu}}}\\sim t_\\nu\\]\nhas a \\(t-student\\) distribution with \\(\\nu\\) degrees of freedom, i.e. \\(T\\sim t_\\nu\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#properties-of-t-student",
    "href": "slides/02_sampling_distributions.html#properties-of-t-student",
    "title": "Sampling Distributions",
    "section": "Properties of t-Student",
    "text": "Properties of t-Student\nKey Properties:\n\nSymmetric around zero: \\(E[T] = 0\\) (for \\(\\nu &gt; 1\\))\nVariance: \\(Var[T] = \\frac{\\nu}{\\nu - 2}\\) (for \\(\\nu &gt; 2\\))\nAs \\(\\nu \\to \\infty\\): \\(t_\\nu \\to N(0,1)\\)\nHeavier tails than the standard normal distribution\nShape depends on degrees of freedom \\(\\nu\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#t-student-vs.-normal-distribution",
    "href": "slides/02_sampling_distributions.html#t-student-vs.-normal-distribution",
    "title": "Sampling Distributions",
    "section": "t-Student vs.¬†Normal Distribution",
    "text": "t-Student vs.¬†Normal Distribution\n\nThe t-distribution (red) has heavier tails than the Normal (dashed). As \\(\\nu\\) increases, it approaches the Normal distribution."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#using-t-student-tables",
    "href": "slides/02_sampling_distributions.html#using-t-student-tables",
    "title": "Sampling Distributions",
    "section": "Using t-Student Tables",
    "text": "Using t-Student Tables\nFor a t-Student distribution with \\(\\nu\\) degrees of freedom:\n\nTables typically provide: \\(P(T &gt; t_{\\alpha, \\nu}) = \\alpha\\)\nDue to symmetry: \\(t_{1-\\alpha, \\nu} = -t_{\\alpha, \\nu}\\)\n\n\nExample: For \\(\\nu = 10\\) and \\(\\alpha = 0.05\\):\n\n\\(t_{0.05, 10} = 1.812\\)\nThis means: \\(P(T &gt; 1.812) = 0.05\\)\nBy symmetry: \\(P(T &lt; -1.812) = 0.05\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-2",
    "href": "slides/02_sampling_distributions.html#exercise-2",
    "title": "Sampling Distributions",
    "section": "Exercise",
    "text": "Exercise\nConsider the rv \\(T\\sim t_{11}\\).\n\nT/F For \\(P(T\\leq k)=0.75\\), it must be that \\(k=0.303\\).\n\\(P(|T|&gt;2.201)\\) is:\n\n\\(0.025\\)\n\\(0.95\\)\n\\(0.05\\)\nNone of the above."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-3",
    "href": "slides/02_sampling_distributions.html#exercise-3",
    "title": "Sampling Distributions",
    "section": "Exercise",
    "text": "Exercise\n\n\\(P(T\\leq k)=0.75\\ \\Leftrightarrow\\ P(T&gt;k)=0.25\\ \\Leftrightarrow\\ k=0.697\\)\n\\(P(|T|&gt;2.201)=2P(T&gt;2.201)=2\\times 0.025=0.05\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#case-1-normal-population-sigma-known",
    "href": "slides/02_sampling_distributions.html#case-1-normal-population-sigma-known",
    "title": "Sampling Distributions",
    "section": "Case 1: Normal Population, \\(\\sigma\\) Known",
    "text": "Case 1: Normal Population, \\(\\sigma\\) Known\n\nSetup:\n\nPopulation: \\(X \\sim N(\\mu, \\sigma)\\) with \\(\\sigma^2\\) known\nRandom sample: \\((X_1, X_2, \\ldots, X_n)\\) where \\(X_i \\sim N(\\mu, \\sigma)\\)\nStatistic: Sample mean \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#case-1-normal-population-sigma-known-1",
    "href": "slides/02_sampling_distributions.html#case-1-normal-population-sigma-known-1",
    "title": "Sampling Distributions",
    "section": "Case 1: Normal Population, \\(\\sigma\\) Known",
    "text": "Case 1: Normal Population, \\(\\sigma\\) Known\nSampling Distribution:\nBy the additivity theorem for Normal distributions:\n\\[\\bar{X} \\sim N\\left(\\mu, \\frac{\\sigma}{\\sqrt{n}}\\right) \\Leftrightarrow Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#case-2-normal-population-sigma-unknown",
    "href": "slides/02_sampling_distributions.html#case-2-normal-population-sigma-unknown",
    "title": "Sampling Distributions",
    "section": "Case 2: Normal Population, \\(\\sigma\\) Unknown",
    "text": "Case 2: Normal Population, \\(\\sigma\\) Unknown\n\nSetup:\n\nPopulation: \\(X \\sim N(\\mu, \\sigma)\\) with \\(\\sigma^2\\) unknown\nRandom sample: \\((X_1, X_2, \\ldots, X_n)\\)\nStatistic: Sample mean \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#case-2-normal-population-sigma-unknown-1",
    "href": "slides/02_sampling_distributions.html#case-2-normal-population-sigma-unknown-1",
    "title": "Sampling Distributions",
    "section": "Case 2: Normal Population, \\(\\sigma\\) Unknown",
    "text": "Case 2: Normal Population, \\(\\sigma\\) Unknown\nSampling Distribution:\nWe must use the sample standard deviation:\n\\[T = \\frac{\\bar{X} - \\mu}{S/\\sqrt{n-1}} = \\frac{\\bar{X} - \\mu}{S'/\\sqrt{n}} \\sim t_{n-1}\\]\nwhere \\(S' = \\sqrt{\\frac{n}{n-1}}S\\) is the corrected sample standard deviation."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#important-note",
    "href": "slides/02_sampling_distributions.html#important-note",
    "title": "Sampling Distributions",
    "section": "Important Note",
    "text": "Important Note\n\n\n\n\n\n\nWhen to Use t vs.¬†Z\n\n\n\nIt‚Äôs rare to know \\(\\sigma^2\\) without knowing \\(\\mu\\)\nIn practice, we almost always use the t-Student distribution\nWe estimate \\(\\sigma\\) using the sample standard deviation \\(S\\) or \\(S'\\)\nThe t-distribution accounts for the uncertainty from estimating \\(\\sigma\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#important-note-1",
    "href": "slides/02_sampling_distributions.html#important-note-1",
    "title": "Sampling Distributions",
    "section": "Important Note",
    "text": "Important Note\nThus:\n\\[S^2=\\frac{1}{n}\\sum_{i=1}^n (X_i-\\bar{X})^2\\] and\n\\[S'^2=\\frac{1}{n-1}\\sum_{i=1}^n (X_i-\\bar{X})^2=\\frac{n}{n-1}S^2\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#important-note-2",
    "href": "slides/02_sampling_distributions.html#important-note-2",
    "title": "Sampling Distributions",
    "section": "Important Note",
    "text": "Important Note\nIf we have a large enough sample (larger than 30), we can, from the sample, approximate the distribution of \\(\\mu\\), using the TLC (again, forthcoming):\n\\[Z=\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n-1}}}=\\frac{\\bar{X}-\\mu}{\\frac{S'}{\\sqrt{n}}}\\overset{\\cdot}{\\sim}N(0,1)\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#case-3-unknown-population-large-sample",
    "href": "slides/02_sampling_distributions.html#case-3-unknown-population-large-sample",
    "title": "Sampling Distributions",
    "section": "Case 3: Unknown Population, Large Sample",
    "text": "Case 3: Unknown Population, Large Sample\n\nSetup:\n\nPopulation: Unknown distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) known\nRandom sample: \\((X_1, X_2, \\ldots, X_n)\\) with \\(n \\geq 30\\)\nStatistic: Sample mean \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#case-3-unknown-population-large-sample-1",
    "href": "slides/02_sampling_distributions.html#case-3-unknown-population-large-sample-1",
    "title": "Sampling Distributions",
    "section": "Case 3: Unknown Population, Large Sample",
    "text": "Case 3: Unknown Population, Large Sample\nApproximate Sampling Distribution (by CLT):\nFor \\(n\\) sufficiently large (\\(n \\geq 30\\)), by the Central Limit Theorem:\n\\[Z = \\frac{\\bar{X} - \\mu}{\\sigma/\\sqrt{n}} \\underbrace{\\overset{\\cdot}{\\sim}}_{CLT} N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#case-3-unknown-population-large-sample-2",
    "href": "slides/02_sampling_distributions.html#case-3-unknown-population-large-sample-2",
    "title": "Sampling Distributions",
    "section": "Case 3: Unknown Population, Large Sample",
    "text": "Case 3: Unknown Population, Large Sample\n\n\n\n\n\n\nNote:\n\n\nIf \\(\\sigma^2\\) is unknown, the aproximate sampling distribution is given by: \\[Z=\\frac{\\bar{X}-\\mu}{\\frac{S}{\\sqrt{n-1}}}=\\frac{\\bar{X}-\\mu}{\\frac{S'}{\\sqrt{n}}}\\underbrace{\\overset{\\cdot}{\\sim}}_{CLT}N(0,1)\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#central-limit-theorem-clt",
    "href": "slides/02_sampling_distributions.html#central-limit-theorem-clt",
    "title": "Sampling Distributions",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\n\n\n\nCentral Limit Theorem\n\n\nRegardless of the population distribution, as the sample size \\(n\\) increases, the sampling distribution of \\(\\bar{X}\\) approaches a Normal distribution with:\n\nMean: \\(E[\\bar{X}] = \\mu\\)\nStandard Deviation: \\(\\sqrt{V[\\bar{X}]} = \\sigma/\\sqrt{n}\\)\n\nThis is one of the most important theorems in statistics!"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#case-3-continued-sigma-unknown-large-sample",
    "href": "slides/02_sampling_distributions.html#case-3-continued-sigma-unknown-large-sample",
    "title": "Sampling Distributions",
    "section": "Case 3 (Continued): \\(\\sigma\\) Unknown, Large Sample",
    "text": "Case 3 (Continued): \\(\\sigma\\) Unknown, Large Sample\nWhen \\(\\sigma\\) is unknown and \\(n &gt; 30\\):\n\\[Z = \\frac{\\bar{X} - \\mu}{S/(n-1)^{1/2}} = \\frac{\\bar{X} - \\mu}{S'/\\sqrt{n}} \\overset{CLT}{\\sim} N(0, 1)\\]\n\nThe t-distribution with many degrees of freedom is approximately Normal\nFor practical purposes, \\(t_{n-1} \\approx N(0,1)\\) when \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#summary-which-distribution-to-use",
    "href": "slides/02_sampling_distributions.html#summary-which-distribution-to-use",
    "title": "Sampling Distributions",
    "section": "Summary: Which Distribution to Use?",
    "text": "Summary: Which Distribution to Use?\n\n\n\nPopulation\n\\(\\sigma\\)\nSample Size\nUse\n\n\n\n\nNormal\nKnown\nAny \\(n\\)\n\\(Z \\sim N(0,1)\\)\n\n\nNormal\nUnknown\nAny \\(n\\)\n\\(T \\sim t_{n-1}\\)\n\n\nNormal\nUnknown\n\\(n &gt; 30\\)\n\\(Z \\sim N(0,1)\\) (approx.)\n\n\nUnknown\nKnown\n\\(n \\geq 30\\)\n\\(Z \\sim N(0,1)\\) (CLT)\n\n\nUnknown\nUnknown\n\\(n &gt; 30\\)\n\\(Z \\sim N(0,1)\\) (CLT)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-7",
    "href": "slides/02_sampling_distributions.html#exercise-7",
    "title": "Sampling Distributions",
    "section": "Exercise 7",
    "text": "Exercise 7\nA company wishes to validate the value (in euros) of its accounts receivable from clients, assuming the underlying distribution of account values is \\(\\text{Normal}(\\mu = 385,\\; \\sigma = ?)\\).\nA random sample of 25 client accounts (independent) was selected, yielding a corrected sample standard deviation \\(s' = 122.6\\) euros.\n¬†\na) \\(\\boxed{V}\\) \\(\\boxed{F}\\) The sampling distribution of the mean value of client accounts is \\(t_{(25)}\\).\nb) \\(\\boxed{V}\\) \\(\\boxed{F}\\) The required sample size (assuming the corrected sample standard deviation remains the same) so that the sample mean does not deviate from the population mean by more than 20 euros in 90% of cases is \\(n = 102\\). (Assume that the value of \\(n\\) to be found will be greater than 30.)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-7-solution-a",
    "href": "slides/02_sampling_distributions.html#exercise-7-solution-a",
    "title": "Sampling Distributions",
    "section": "Exercise 7 ‚Äî Solution a)",
    "text": "Exercise 7 ‚Äî Solution a)\nPopulation: \\(X \\sim N(\\mu = 385,\\; \\sigma = ?)\\) (standard deviation unknown)\nSample: \\(n = 25\\); \\(s' = 122.6\\)\n¬†\nThe sampling distribution of the sample mean (mean value of client accounts) is given by:\n\\[\nT = \\frac{\\bar{X} - \\mu}{\\dfrac{S'}{\\sqrt{n}}} \\sim t_{(n-1)} \\equiv t_{(24)}\n\\]\nThe statement claims \\(t_{(25)}\\), but the correct distribution is \\(t_{(24)}\\).\nThe statement is False."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-7-solution-b",
    "href": "slides/02_sampling_distributions.html#exercise-7-solution-b",
    "title": "Sampling Distributions",
    "section": "Exercise 7 ‚Äî Solution b)",
    "text": "Exercise 7 ‚Äî Solution b)\nPopulation: \\(X \\sim N(\\mu = 385,\\; \\sigma = ?)\\) | Sample: \\(n \\geq 30\\); \\(s' = 122.6\\)\nSince \\(n \\geq 30\\), by the CLT:\n\\[\n\\bar{X} \\;\\underset{\\text{approx. by CLT}}{\\sim}\\; N\\!\\left(\\mu,\\; \\frac{s'}{\\sqrt{n}}\\right) \\equiv N\\!\\left(385,\\; \\frac{122.6}{\\sqrt{n}}\\right)\n\\]\nWe want \\(P(|\\bar{X} - \\mu| &lt; 20) = 0.9\\):\n\\[\nP\\!\\left(|Z| &lt; \\frac{20}{\\dfrac{122.6}{\\sqrt{n}}}\\right) = 0.9 \\;\\Leftrightarrow\\; P\\!\\left(|Z| &gt; \\frac{20}{\\dfrac{122.6}{\\sqrt{n}}}\\right) = 0.1\n\\]\n\\[\n\\Leftrightarrow\\; \\frac{20}{\\dfrac{122.6}{\\sqrt{n}}} = 1.645 \\;\\Leftrightarrow\\; n \\geq 102\n\\]\nThe statement is True."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#distribution-of-sample-variance",
    "href": "slides/02_sampling_distributions.html#distribution-of-sample-variance",
    "title": "Sampling Distributions",
    "section": "Distribution of Sample Variance",
    "text": "Distribution of Sample Variance\n\nSetup:\n\nPopulation: \\(X \\sim N(\\mu, \\sigma)\\) with \\(\\mu\\) unknown and variance \\(\\sigma^2\\)\nRandom sample: \\((X_1, X_2, \\ldots, X_n)\\) where \\(X_i \\sim N(\\mu, \\sigma)\\)\nStatistics:\n\nSample variance: \\(S^2 = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\)\nCorrected sample variance: \\(S'^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\bar{X})^2 = \\frac{n}{n-1}S^2\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#sampling-distribution-of-variance",
    "href": "slides/02_sampling_distributions.html#sampling-distribution-of-variance",
    "title": "Sampling Distributions",
    "section": "Sampling Distribution of Variance",
    "text": "Sampling Distribution of Variance\nResult:\n\\[Q = \\frac{(n-1)S'^2}{\\sigma^2} = \\frac{nS^2}{\\sigma^2} \\sim \\chi^2_{n-1}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe chi-square distribution has \\(n-1\\) degrees of freedom\nWe lose one degree of freedom because we estimate \\(\\mu\\) with \\(\\bar{X}\\)\nThis result is only exact for Normal populations"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#why-do-we-lose-a-degree-of-freedom",
    "href": "slides/02_sampling_distributions.html#why-do-we-lose-a-degree-of-freedom",
    "title": "Sampling Distributions",
    "section": "Why Do We Lose a Degree of Freedom?",
    "text": "Why Do We Lose a Degree of Freedom?\nWhen calculating \\(S^2 = \\frac{1}{n}\\sum_{i=1}^{n}(X_i - \\bar{X})^2\\):\n\nWe have \\(n\\) observations: \\(X_1, X_2, \\ldots, X_n\\)\nBut we use \\(\\bar{X}\\) (calculated from the data) instead of \\(\\mu\\)\nThe \\(n\\) deviations \\((X_i - \\bar{X})\\) must sum to zero\nTherefore, only \\(n-1\\) of them are ‚Äúfree‚Äù to vary\nWe say we have \\(n-1\\) degrees of freedom"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-8",
    "href": "slides/02_sampling_distributions.html#exercise-8",
    "title": "Sampling Distributions",
    "section": "Exercise 8",
    "text": "Exercise 8\nA pharmaceutical company produces tablets in which the variability of the amount of active substance from one tablet to another must be very small. The population standard deviation is supposedly one milligram. Inspectors from the Ministry of Health selected a random sample of 16 tablets.\nAssuming the population is Normal, the probability that the corrected sample variance exceeds \\(0.736\\;\\text{mg}^2\\) is:\n¬†\na) 0.76 ‚ÄÉ b) 0.25 ‚ÄÉ c) 0.75 ‚ÄÉ d) none of the above"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-8-solution",
    "href": "slides/02_sampling_distributions.html#exercise-8-solution",
    "title": "Sampling Distributions",
    "section": "Exercise 8 ‚Äî Solution",
    "text": "Exercise 8 ‚Äî Solution\nPopulation: \\(X \\sim N(\\mu,\\; \\sigma)\\), with \\(\\sigma = 1\\) (i.e.¬†\\(\\sigma^2 = 1\\))\nSample: \\(n = 16\\)\nPivot quantity:\n\\[\nQ = \\frac{(n-1)\\,S'^2}{\\sigma^2} = \\frac{15\\,S'^2}{1} = 15\\,S'^2 \\sim \\chi^2_{(n-1)} \\equiv \\chi^2_{(15)}\n\\]\nWe want:\n\\[\nP(S'^2 &gt; 0.736) = P(Q &gt; 15 \\times 0.736) = P(Q &gt; 11.04)\n\\]\nReading directly from the \\(\\chi^2\\) table with 15 degrees of freedom:\n\\[\nP(Q &gt; 11.04) \\approx 0.75\n\\]\nThe correct answer is c)."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#bernoulli-population",
    "href": "slides/02_sampling_distributions.html#bernoulli-population",
    "title": "Sampling Distributions",
    "section": "Bernoulli Population",
    "text": "Bernoulli Population\n\nSetup:\n\nPopulation: \\(X \\sim Bernoulli(p)\\) where\n\n\\(P(X=1) = p\\) (success)\n\\(P(X=0) = 1-p = q\\) (failure)\n\nParameters: \\(E[X] = p\\) and \\(V[X] = p(1-p) = pq\\)\nRandom sample: \\((X_1, X_2, \\ldots, X_n)\\)"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#sample-proportion",
    "href": "slides/02_sampling_distributions.html#sample-proportion",
    "title": "Sampling Distributions",
    "section": "Sample Proportion",
    "text": "Sample Proportion\nThe sample proportion is just the sample mean for a Bernoulli population:\n\\[\\hat{p} = \\bar{X} = \\frac{X_1 + X_2 + \\cdots + X_n}{n} = \\frac{\\text{number of successes}}{n}\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#first-case",
    "href": "slides/02_sampling_distributions.html#first-case",
    "title": "Sampling Distributions",
    "section": "First case",
    "text": "First case\nLet \\(Y=\\sum_{i=1}^n X_i\\), the total amount of successes. Then \\(Y\\) has a sample distribution:\n\\[Y=\\sum_{i=1}^m\\sim Binomial(n,p)\\]\nIf \\(n\\) is large enough, we can apply the \\(CLT\\) to get an approximate sample distribution: \\[Y\\overset{\\cdot}{\\sim} N\\left(np,\\sqrt{np(1-p)}\\right)\\Leftrightarrow Z_n=\\frac{Y-np}{\\sqrt{np(1-p)}}\\overset{\\cdot}{\\sim}N(0,1)\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#second-case",
    "href": "slides/02_sampling_distributions.html#second-case",
    "title": "Sampling Distributions",
    "section": "Second case",
    "text": "Second case\nLet the statistic \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^n X_i = \\frac{Y}{n}\\) the proportion of successess that, in a sample of size \\(n\\), have some characteristic.\nIt is of interest to know the sampling distribution of \\(\\bar{X}=\\hat{p}\\)\nExpected Value and Variance:\n\\[E[\\bar{X}] = E[\\hat{p}] = p\\]\n\\[V[\\bar{X}] = V[\\hat{p}] = \\frac{p(1-p)}{n} = \\frac{pq}{n}\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#approximate-distribution-by-clt",
    "href": "slides/02_sampling_distributions.html#approximate-distribution-by-clt",
    "title": "Sampling Distributions",
    "section": "Approximate Distribution (by CLT)",
    "text": "Approximate Distribution (by CLT)\nWhen \\(n \\to \\infty\\), the Central Limit Theorem gives us:\n\\[\\bar{X} = \\hat{p} \\overset{CLT}{\\sim} N\\left(p, \\sqrt{\\frac{p(1-p)}{n}}\\right)\\]\nEquivalently:\n\\[Z_n = \\frac{\\bar{X} - p}{\\sqrt{p(1-p)/n}} = \\frac{\\hat{p} - p}{\\sqrt{pq/n}} \\overset{CLT}{\\sim} N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-10",
    "href": "slides/02_sampling_distributions.html#exercise-10",
    "title": "Sampling Distributions",
    "section": "Exercise 10",
    "text": "Exercise 10\nIn a certain neighborhood, the proportion of residents who consider useful the opening of a new supermarket is 0.8. The Sweet Drop supermarket chain intends to open a new store in this neighborhood. Nevertheless, before making a decision, a study was conducted in which 400 residents were casually interviewed to assess their intention to use the services of the new supermarket.\na) \\(\\boxed{V}\\) \\(\\boxed{F}\\) The approximate probability that in the sample of interviewed residents there are at least 300 who consider useful the opening of the Sweet Drop supermarket is equal to 0.9948.\nb) \\(\\boxed{V}\\) \\(\\boxed{F}\\) The probability that the deviation between the sample proportion of residents who stated they would actually use the services of the new supermarket and the true proportion is less than 0.02 is 0.8413."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-10-solution-a",
    "href": "slides/02_sampling_distributions.html#exercise-10-solution-a",
    "title": "Sampling Distributions",
    "section": "Exercise 10 ‚Äî Solution a)",
    "text": "Exercise 10 ‚Äî Solution a)\n\\(p = 0.8\\) (proportion of residents who consider the opening useful)\na) \\(X \\to\\) number of residents, out of 400, who find the opening of the new supermarket useful\n\\[X \\sim \\text{Binomial}(n = 400,\\; p = 0.8)\\]\n\\[\n\\begin{align}\nP(X \\geq 300) &= P(X \\geq 299.5) \\;\\underset{1)}{\\approx}\\; P\\!\\left(Z \\geq \\frac{299.5 - 400 \\times 0.8}{\\sqrt{400 \\times 0.8 \\times 0.2}}\\right) \\\\[8pt]\n&= P(Z \\geq -2.56) = P(Z \\leq 2.56) = \\Phi(2.56) = 0.9948\n\\end{align}\n\\]\nObs:\n\nContinuity correction\nCLT, approximation of the Binomial distribution to the Normal distribution\n\n\\[Z = \\frac{X - np}{\\sqrt{np(1-p)}} \\sim N(0,1)\\]\nThe statement is true."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#exercise-10-solution-b",
    "href": "slides/02_sampling_distributions.html#exercise-10-solution-b",
    "title": "Sampling Distributions",
    "section": "Exercise 10 ‚Äî Solution b)",
    "text": "Exercise 10 ‚Äî Solution b)\nb) From the interviews conducted, \\(X\\) residents out of \\(n\\) stated they would use Sweet Drop, thus the sample proportion is \\(\\dfrac{X}{n}\\).\n\\[\n\\begin{align}\nP\\!\\left(\\left|\\frac{X}{n} - p\\right| &lt; 0.02\\right) &\\;\\underset{1)}{\\approx}\\; P\\!\\left(|Z| &lt; \\frac{0.02}{\\sqrt{\\dfrac{0.8 \\times 0.2}{400}}}\\right) = P(|Z| &lt; 1) \\\\[8pt]\n&= P(-1 &lt; Z &lt; 1) = \\Phi(1) - \\Phi(-1) = \\Phi(1) - [1 - \\Phi(1)] \\\\[8pt]\n&= 2\\Phi(1) - 1 = 2 \\times 0.8413 - 1 = 0.6826\n\\end{align}\n\\]\nObs: 1) Note that we have \\(\\dfrac{x - np}{\\sqrt{np(1-p)}} = \\dfrac{\\dfrac{x}{n} - p}{\\sqrt{\\dfrac{p(1-p)}{n}}} = \\dfrac{\\dfrac{x}{n} - p}{\\sqrt{\\dfrac{p(1-p)}{n}}}\\) and that, by CLT,\n\\[Z = \\frac{X - np}{\\sqrt{np(1-p)}} \\sim N(0,1) \\text{ and therefore, also } Z = \\frac{\\dfrac{X}{n} - p}{\\sqrt{\\dfrac{p(1-p)}{n}}} \\sim N(0,1)\\]\nThe statement is false."
  },
  {
    "objectID": "slides/02_sampling_distributions.html#summary-sampling-distributions",
    "href": "slides/02_sampling_distributions.html#summary-sampling-distributions",
    "title": "Sampling Distributions",
    "section": "Summary: Sampling Distributions",
    "text": "Summary: Sampling Distributions\nWhat We Learned:\n\nStatistics are functions of sample data with no unknown parameters\nt-Student distribution is used when \\(\\sigma\\) is unknown (Normal population)\nChi-square distribution describes the sampling distribution of variance\nCentral Limit Theorem allows us to use Normal approximation for large samples\nSample proportions from Bernoulli populations are approximately Normal (large \\(n\\))"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#practical-guidelines",
    "href": "slides/02_sampling_distributions.html#practical-guidelines",
    "title": "Sampling Distributions",
    "section": "Practical Guidelines",
    "text": "Practical Guidelines\nChoosing the Right Distribution:\n\nSmall sample, Normal pop., \\(\\sigma\\) unknown ‚Üí Use \\(t_{n-1}\\)\nLarge sample (\\(n &gt; 30\\)) ‚Üí Use \\(N(0,1)\\) (CLT)\nVariance problems, Normal pop. ‚Üí Use \\(\\chi^2_{n-1}\\)\nProportions, large sample ‚Üí Use \\(N(0,1)\\) (CLT)\n\n\n\n\n\n\n\nImportant\n\n\nAlways check assumptions before applying these results!"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#next-steps",
    "href": "slides/02_sampling_distributions.html#next-steps",
    "title": "Sampling Distributions",
    "section": "Next Steps",
    "text": "Next Steps\nIn the next lectures:\n\nPoint Estimation\nConfidence Intervals\nHypothesis Testing\n\nThese all build on our understanding of sampling distributions!"
  },
  {
    "objectID": "slides/02_sampling_distributions.html#references",
    "href": "slides/02_sampling_distributions.html#references",
    "title": "Sampling Distributions",
    "section": "References",
    "text": "References\n:book: Murteira, B.; Silva Ribeiro, C.; Andrade e Silva, J. & Pimenta, C. (2010). Introdu√ß√£o √† Estat√≠stica. Escolar Editora, McGraw-Hill.\n:book: Newbold, P., Carlson, W., & Thorne, B. (2022). Statistics for Business and Economics Global Edition (10th Ed.). Pearson."
  },
  {
    "objectID": "material.html",
    "href": "material.html",
    "title": "Material",
    "section": "",
    "text": "Syllabus\n\nFormulae\n\nCalendar"
  },
  {
    "objectID": "material.html#core-material",
    "href": "material.html#core-material",
    "title": "Material",
    "section": "",
    "text": "Syllabus\n\nFormulae\n\nCalendar"
  },
  {
    "objectID": "material.html#slides",
    "href": "material.html#slides",
    "title": "Material",
    "section": "Slides",
    "text": "Slides\n\nCentral Limit Theorem\nSampling Distributions"
  },
  {
    "objectID": "material.html#exercises",
    "href": "material.html#exercises",
    "title": "Material",
    "section": "Exercises",
    "text": "Exercises\n\nCentral Limit Theorem\nSamling Distributions"
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Statistics II ‚Äî Calendar",
    "section": "",
    "text": "Week\nDate\nLecture\nTopic (from syllabus)\n\n\n\n\n1\nFev 24\n1-2\nSampling Distributions: Central Limit Theorem (1.1-1.3)\n\n\n2\nMar 10\n3-4\nSampling Distributions (1.4-1.6)\n\n\n3\nMar 17\n5-6\nParameter Estimation: Point Estimation (2.1)\n\n\n4\nMar 24\n7-8\nTest 1\n\n\n5\nApr 7\n9-10\nParameter Estimation: Interval Estimation (2.2)\n\n\n6\nApr 14\n11-12\nParametric Hypothesis Tests (3.1-3.2)\n\n\n7\nApr 21\n13-14\nParametric Hypothesis Tests (3.3-3.5)\n\n\n8\nApr 28\n15-16\nLinear Regression Model: Estimation (4.1-4.3)\n\n\n9\nMay 5\n17-18\nLinear Regression Model (4.4-4.5)\n\n\n10\nMay 12\n19-20\nLinear Regression Model: Inference (4.6-4.7)\n\n\n11\nMay 19\n21-22\nTest 2"
  },
  {
    "objectID": "exercises/01_Central_Limit_Theorem.html",
    "href": "exercises/01_Central_Limit_Theorem.html",
    "title": "Chapter I - Statistics II - Central Limit Theorem",
    "section": "",
    "text": "This is a set of exercises for Statistics II, Chapter II ‚Äî Sampling Distributions, from the Lisbon Accounting and Business School (ISCAL‚ÄìIPL).\nContents: t-Student and Chi-Squared distributions. Random sample and statistic. Sampling distributions: of the mean in Normal and non-Normal populations, of the variance in Normal populations, and of the proportion from a Bernoulli population.\nThe exercise set consists of True/False questions (where you should determine the truth value and justify your choice) and multiple choice questions (where you should select the correct option and justify it appropriately)."
  },
  {
    "objectID": "exercises/01_Central_Limit_Theorem.html#questions",
    "href": "exercises/01_Central_Limit_Theorem.html#questions",
    "title": "Chapter I - Statistics II - Central Limit Theorem",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\n\nThe number of access to a certain website follows a Poisson distribution with an average rate of \\(30\\) per day.\n\nTRUEFALSE The probability (exact) that, in a a given day, the number of access is higher than 40 is exactly \\(0.0323\\). The approximate probability, by using the CLT, leads to an error of \\(0.0049\\), from the exact result.\nThe probability that the number of accesses in a week (7 days) is between 200 and 220 (inclusive) is approximately: 0.50980.48790.4908None of the above\n\n\n\n\nQuestion 2\n\nA complex system is formed by 100 parts that work indepedently. The probability that any of the components fails during the operating period is equal to \\(0.1\\).\n\nTRUEFALSE Knowing that the working of the system demands that at least 85 parts are operative, the approximate probability that the system works is equal to 0.9664.\nTRUEFALSE In the previous calculus, the use of the CLT demands only that the sample size, from a non Normal population is large enough.\n\n\n\n\nQuestion 3\n\nIt is known that, in the TeleDigit, the probability that any phone call is to fill a complaint is equal to 0.05. Consider that the random experiment, consisting in picking a random sample of 20 calls, and count the ones that are complaints.\n\nTRUEFALSE The probability that in the sample there are between 2 and 5 (inclusive) is equal to 0.2639.\nThe approximate probability that, in a random sample of 250 calls, at most 10 complaints are detected is: 0.29090.28100.2327None of the above\n\n\n\n\nQuestion 4\n\nThe number of machines ordered every day to a factory can be considered a random variable distributed Poisson with mean 9.5.\n\nThe percentage of days in which no more than 10 machines are ordered is equal to: 64.53%12.35%35.47%87.65%\nTRUEFALSE Knowing that the exact probability of having being sent less than 30 machines in 4 work days is 0.097964, the approximation (by the CLT) error is equal to 0.00416.\n\n\n\n\nQuestion 5\n\nThe lifetime, in years, of a lightning projector is a random variable. It is known that 20% of the projectors fail during the first year of use.\nIn a random sample of 500 projectors, the approximate probability that at most 80 fail during the first year is: 0.98540.01250.0146None of the above\n\n\n\nQuestion 6\n\nConsider the time interval that passess between the arrival of two people to the reception desk of a hospital follows an exponential distribution with mean of 30 seconds.\n\nTRUEFALSE The approximate probability (by the CLT) that at most 40 people arrive to the reception desk of the hospital, during a time lapse of 18 minutes is 0.7734.\n\n\n\n\nQuestion 7\n\nIn a gas station, each customer fills a random amount of gas \\(X\\), that it is known to follow a Uniform distribution in \\([10,40]\\) liters. Assume that, each day, the gas station has 300 customers.\nTRUEFALSE The approximate probability that the daily (total) gas bought in that gas station is above 7410 liters of gas is equal to \\(\\Phi(0.6)\\).\n\n\n\nQuestion 8\n\nA study about the sales in the sumermarket Superstore arrived to the conclusion that the daily rice demand (in Kg) is a random variable with mean 40Kg and standard deviation of 5Kg.\nTRUEFALSE Having being ordered 14500 Kg of rice for sale in the next year, the probability that that stock satisfies the rice demand, in that year, is equal to 0.2643 (consider a year with 364 days)."
  },
  {
    "objectID": "exercises/02_Sampling_Distributions.html",
    "href": "exercises/02_Sampling_Distributions.html",
    "title": "Chapter I - Statistics II - Sampling Distributions",
    "section": "",
    "text": "This is a set of exercises for Statistics II, Chapter II ‚Äî Sampling Distributions, from the Lisbon Accounting and Business School (ISCAL‚ÄìIPL).\nContents: t-Student and Chi-Squared distributions. Random sample and statistic. Sampling distributions: of the mean in Normal and non-Normal populations, of the variance in Normal populations, and of the proportion from a Bernoulli population.\nThe exercise set consists of True/False questions (where you should determine the truth value and justify your choice) and multiple choice questions (where you should select the correct option and justify it appropriately)."
  },
  {
    "objectID": "exercises/02_Sampling_Distributions.html#questions",
    "href": "exercises/02_Sampling_Distributions.html#questions",
    "title": "Chapter I - Statistics II - Sampling Distributions",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\n\nLet \\((X_1,X_2,\\ldots,X_n)\\) with \\(n\\in\\mathbb{N}\\), a random sample from a population distributed \\(X\\sim Normal(\\mu=-2,\\sigma)\\), where \\(\\sigma\\) is unknown. Asses if, the following random variables are not statistics, i.e., if you select true, you are claiming that the random variable is not a statistic.\n\nTRUEFALSE \\(T(X_1,X_2,\\ldots,X_n)=X_1+X_2+\\ldots+X_n\\)\nTRUEFALSE \\(T(X_1,X_2,\\ldots,X_n)= \\frac{X_1+X_2+\\ldots+X_n}{n}=\\bar{X}\\)\nTRUEFALSE \\(T(X_1,X_2,\\ldots,X_n)= \\frac{\\bar{X}}{\\sigma}\\)\nTRUEFALSE \\(T(X_1,X_2,\\ldots,X_n)= \\frac{\\bar{X}-\\mu}{\\sigma}\\sqrt{n}\\)\nTRUEFALSE \\(T(X_1,X_2,\\ldots,X_n)= \\frac{\\sum_{i=1}^n(X_i-\\mu)^2}{n}\\)\nTRUEFALSE \\(T(X_1,X_2,\\ldots,X_n)= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})^2}{n}=S^2\\)\nTRUEFALSE \\(T(X_1,X_2,\\ldots,X_n)= \\frac{\\sum_{i=1}^n(X_i-\\bar{X})^2}{n-1}=S'^2\\)\n\n\n\n\nQuestion 2\n\nLet the waiting time for take-off of each plane in a given airport is a r.v. distributed \\(N(\\mu=4,\\sigma)\\). Randomly chosen 21 flights, the records allow to estimate \\(s'=1.81min\\).\nTRUEFALSE The probability that the average waiting time, from the sample, is larger than 5 minutes equals \\(0.010\\).\n\n\n\nQuestion 3\n\nIn a given financial firm, the profits obtained from each investment, in the last year, average 8,200 euro, and a standard deviation of 3,600 euro. Consider a random sample of 100 investments. The probability that the sample average is farther away from the true mean by 806.76 euro, is: 0.01250.0250.9875None of the previous\n\n\n\nQuestion 4\n\nA given company intends to check the value (in euro) of the receivable accounts, with an underlying statistical behavior that follows a Normal distribution with mean \\(\\mu=385\\) and standard deviation \\(\\sigma\\). Choosing randomly a sample with \\(n\\) accounts (independently), you obtained a (corrected) sample standard deviation of \\(122.6\\) euro.\nTRUEFALSE The size of the sample to collect (assuming that the estimate for \\(s'\\) is maintained) such that the sample mean is not farther from the true mean than 20 eur in 90% of the cases is \\(n=102\\). (Assume for your computations, that the value of \\(n\\) will be larger than 30)\n\n\n\nQuestion 5\n\nAssume that a particular streaming service is subscribed by 10% of the population in a given country. Because this service is very recent, a marketing firm decided to estimate that proportion, based in a sample of 100 citizens, before renewing their contract with the streaming service company.\nTRUEFALSE Knowing that the contracts will be renewed only if the sample proportion is larger than 8.5%, the probability tha this contract will be renewed is 0.6915.\n\n\n\nQuestion 6\n\nAssume that you intend to study the feasibility of a project which core variable is the price (in monetary units) of raw-materials, which is well represented by the random variable \\(X\\sim N(\\mu,\\sigma)\\). It was selected, randomly, a sample \\((X_1,\\ldots,X_{16})\\) from that population, where the sample (corrected) variance was found to be \\(0.18 um^2\\).\n\nTRUEFALSE \\(Q=\\frac{15S'^2}{\\sigma^2}\\sim \\chi^2_{16}\\)\nTRUEFALSE \\(P(S'2^2&gt;1.2163 \\sigma^2)=0.25\\)\n\n\n\n\nQuestion 7\n\n15 adult males, with ages between 35 and 50, participated in a study to assess the effect of diet and exercises in the cholesterol levels found in blood. The cholesterol was measured at the begining and after three months after participating of a program that consisted in a new diet and aerobic exercises. The data is found in the following table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n\n\n\n\nBefore\n265\n240\n258\n295\n251\n245\n287\n314\n260\n279\n283\n240\n238\n225\n247\n\n\nAfter\n229\n231\n227\n240\n238\n241\n234\n256\n247\n239\n246\n218\n219\n226\n233\n\n\n\nAssume that:\n\nThe samples Before and After, are paired (each column is the same individual)\nEach sampled pair, is an independent observation of the other.\nThe cholesterol measures follow and Normal distribution.\n\nTRUEFALSE The probability that the mean of the difference between after and before the treatment is farther away from its mean by more than 6.61 is 0.2."
  },
  {
    "objectID": "exercises/01_Central_Limit_Theorem copy.html",
    "href": "exercises/01_Central_Limit_Theorem copy.html",
    "title": "Chapter I - Statistics II - Central Limit Theorem",
    "section": "",
    "text": "This is a set of exercises for Statistics II, Chapter I ‚Äî Central Limit Theorem, from the Lisbon Accounting and Business School (ISCAL‚ÄìIPL).\nContents: We approach the Central Limit Theorem and use it to approximate other common distributions."
  },
  {
    "objectID": "exercises/01_Central_Limit_Theorem copy.html#questions",
    "href": "exercises/01_Central_Limit_Theorem copy.html#questions",
    "title": "Chapter I - Statistics II - Central Limit Theorem",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\n\nThe number of access to a certain website follows a Poisson distribution with an average rate of \\(30\\) per day.\n\nTRUEFALSE The probability (exact) that, in a a given day, the number of access is higher than 40 is exactly \\(0.0323\\). The approximate probability, by using the CLT, leads to an error of \\(0.0049\\), from the exact result.\nThe probability that the number of accesses in a week (7 days) is between 200 and 220 (inclusive) is approximately: 0.50980.48790.4908None of the above\n\n\n\n\nQuestion 2\n\nA complex system is formed by 100 parts that work indepedently. The probability that any of the components fails during the operating period is equal to \\(0.1\\).\n\nTRUEFALSE Knowing that the working of the system demands that at least 85 parts are operative, the approximate probability that the system works is equal to 0.9664.\nTRUEFALSE In the previous calculus, the use of the CLT demands only that the sample size, from a non Normal population is large enough.\n\n\n\n\nQuestion 3\n\nIt is known that, in the TeleDigit, the probability that any phone call is to fill a complaint is equal to 0.05. Consider that the random experiment, consisting in picking a random sample of 20 calls, and count the ones that are complaints.\n\nTRUEFALSE The probability that in the sample there are between 2 and 5 (inclusive) is equal to 0.2639.\nThe approximate probability that, in a random sample of 250 calls, at most 10 complaints are detected is: 0.29090.28100.2327None of the above\n\n\n\n\nQuestion 4\n\nThe number of machines ordered every day to a factory can be considered a random variable distributed Poisson with mean 9.5.\n\nThe percentage of days in which no more than 10 machines are ordered is equal to: 64.53%12.35%35.47%87.65%\nTRUEFALSE Knowing that the exact probability of having being sent less than 30 machines in 4 work days is 0.097964, the approximation (by the CLT) error is equal to 0.00416.\n\n\n\n\nQuestion 5\n\nThe lifetime, in years, of a lightning projector is a random variable. It is known that 20% of the projectors fail during the first year of use.\nIn a random sample of 500 projectors, the approximate probability that at most 80 fail during the first year is: 0.98540.01250.0146None of the above\n\n\n\nQuestion 6\n\nConsider the time interval that passess between the arrival of two people to the reception desk of a hospital follows an exponential distribution with mean of 30 seconds.\n\nTRUEFALSE The approximate probability (by the CLT) that at most 40 people arrive to the reception desk of the hospital, during a time lapse of 18 minutes is 0.7734.\n\n\n\n\nQuestion 7\n\nIn a gas station, each customer fills a random amount of gas \\(X\\), that it is known to follow a Uniform distribution in \\([10,40]\\) liters. Assume that, each day, the gas station has 300 customers.\nTRUEFALSE The approximate probability that the daily (total) gas bought in that gas station is above 7410 liters of gas is equal to \\(\\Phi(0.6)\\).\n\n\n\nQuestion 8\n\nA study about the sales in the sumermarket Superstore arrived to the conclusion that the daily rice demand (in Kg) is a random variable with mean 40Kg and standard deviation of 5Kg.\nTRUEFALSE Having being ordered 14500 Kg of rice for sale in the next year, the probability that that stock satisfies the rice demand, in that year, is equal to 0.2643 (consider a year with 364 days)."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#random-casual-sample-and-statistic",
    "href": "slides/02_Sampling_Distributions.html#random-casual-sample-and-statistic",
    "title": "Sampling Distributions",
    "section": "Random (Casual) Sample and Statistic",
    "text": "Random (Casual) Sample and Statistic\n\n\nPopulation\n\nParameters (unknown)\n\n\nSample\n\nStatistics (known)\n\n\n\n\nStatistical Inference\n\nStatistical inference consists of a set of methods that allow us to draw conclusions about a population, based on a random (or casual) sample drawn from that population."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#random-sample-parameters-and-statistics",
    "href": "slides/02_Sampling_Distributions.html#random-sample-parameters-and-statistics",
    "title": "Sampling Distributions",
    "section": "Random Sample, Parameters, and Statistics",
    "text": "Random Sample, Parameters, and Statistics\nA sample of size \\(n\\), drawn from a population according to a random (or casual) process, is called a random sample if the values of the random variables \\(X_1, X_2, \\ldots, X_n\\) are independent and identically distributed. The term \\(X_1\\) denotes the first element of the sample, \\(X_2\\) the second, and \\(X_n\\) the \\(n\\)-th.\n\nParameters and Statistics\nA parameter is a value that characterizes a given population which, although unknown, is fixed.\n\n\nA statistic is a characteristic that is a function of the sample values, i.e., it is a random variable that does not involve any unknown parameter, denoted by \\(T(X_1, X_2, \\ldots, X_n)\\)."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#statistics-and-estimates",
    "href": "slides/02_Sampling_Distributions.html#statistics-and-estimates",
    "title": "Sampling Distributions",
    "section": "Statistics and Estimates",
    "text": "Statistics and Estimates\nOnce a particular sample \\(x_1, x_2, \\ldots, x_n\\) has been observed, it is possible to obtain the value of the statistic associated with this sample, namely \\(T(x_1, x_2, \\ldots, x_n)\\), yielding an estimate for the parameter.\n\nFrom each random sample drawn from a population, statistics with different values can be obtained.\n\n\nStatistics are random variables, and therefore their probabilistic behavior is given by their respective distribution function (density or probability function), leading to the sampling distributions of the statistic represented by \\(T(X_1, X_2, \\ldots, X_n)\\)."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-i",
    "href": "slides/02_Sampling_Distributions.html#exercise-i",
    "title": "Sampling Distributions",
    "section": "Exercise I",
    "text": "Exercise I\nConsider \\(X_1, X_2, \\ldots, X_n\\), \\(n \\in \\mathbb{N}\\), a random sample from a \\(Normal(\\mu = -2, \\sigma)\\) population, where \\(\\sigma\\) is unknown. The following r.v.‚Äôs are not statistics:\n\n[T] [F] F \\(\\quad T(X_1, X_2, \\ldots, X_n) = X_1 + X_2 + \\cdots + X_n\\)\n[T] [F] F \\(\\quad T(X_1, X_2, \\ldots, X_n) = \\bar{X}\\)\n[T] [F] T \\(\\quad T(X_1, X_2, \\ldots, X_n) = \\dfrac{\\bar{X}}{\\sigma}\\)\n[T] [F] F \\(\\quad T(X_1, X_2, \\ldots, X_n) = \\displaystyle\\frac{\\sum \\left(X_i - \\mu\\right)^2}{n}\\)\n[T] [F] F \\(\\quad T(X_1, X_2, \\ldots, X_n) = \\dfrac{\\sum (X_i - \\bar{X})^2}{n-1}\\)\n[T] [F] T \\(\\quad T(X_1, X_2, \\ldots, X_n) = \\displaystyle\\sum \\left(\\dfrac{X_i - \\mu}{\\sigma}\\right)^2\\)"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-ii-statement",
    "href": "slides/02_Sampling_Distributions.html#exercise-ii-statement",
    "title": "Sampling Distributions",
    "section": "Exercise II (Statement)",
    "text": "Exercise II (Statement)\nLet \\(X_1, X_2, \\ldots, X_n\\), \\(n \\in \\mathbb{N}\\), be a random sample from a population \\(X\\) (with unknown probabilistic behavior), with expected value \\(\\mu\\) and variance \\(\\sigma^2\\), both known.\n\nThe expected value of the statistics \\(\\displaystyle\\sum_{i=1}^{n} X_i\\) and \\(\\bar{X}\\) are, respectively, equal to: a)\\(n\\mu\\) and \\(\\mu\\) \\(\\quad\\) b) \\(n\\mu\\) and \\(\\mu/n\\) \\(\\quad\\) c) \\(\\mu\\) and \\(\\mu/n\\) \\(\\quad\\) d) none of the above\n\nAnswer: (a)\n\nThe variance of the statistics \\(\\displaystyle\\sum_{i=1}^{n} X_i\\) and \\(\\bar{X}\\) are, respectively, equal to:\n\n\n\\(n\\sigma^2\\) and \\(\\sigma\\) \\(\\quad\\) b)\\(n\\sigma^2\\) and \\(\\sigma^2/n\\) \\(\\quad\\) c) \\(\\sigma^2\\) and \\(\\sigma^2/n\\) \\(\\quad\\) d) none of the above\n\nAnswer: (b)"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-ii-solution",
    "href": "slides/02_Sampling_Distributions.html#exercise-ii-solution",
    "title": "Sampling Distributions",
    "section": "Exercise II (Solution)",
    "text": "Exercise II (Solution)\ni)\n\\[E\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n E(X_i) = \\sum_{i=1}^n \\mu = n\\mu\\]\n\\[E(\\bar{X}) = E\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) = \\frac{1}{n}E\\left(\\sum_{i=1}^n X_i\\right) = \\frac{1}{n} \\times n\\mu = \\mu\\]\nThe correct answer is a).\n\nii)\n\\[V\\left(\\sum_{i=1}^n X_i\\right) = \\sum_{i=1}^n V(X_i) = \\sum_{i=1}^n \\sigma^2 = n\\sigma^2\\]\n\\[V(\\bar{X}) = V\\left(\\frac{1}{n}\\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2}V\\left(\\sum_{i=1}^n X_i\\right) = \\frac{1}{n^2}\\times n\\sigma^2 = \\frac{\\sigma^2}{n}\\]\nThe correct answer is b)."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#chi-square-distribution",
    "href": "slides/02_Sampling_Distributions.html#chi-square-distribution",
    "title": "Sampling Distributions",
    "section": "Chi-Square Distribution",
    "text": "Chi-Square Distribution\nA pertinent question arises: How to obtain the sampling distribution of a statistic or of a random variable?\n\nBefore answering this question, it is important to introduce two fundamental theoretical distributions within the scope of inference.\n\n\nChi-Square Distribution\nLet \\(Z_1, Z_2, \\ldots, Z_n\\) be independent and identically distributed random variables, \\(Z_i \\sim Normal(0, 1)\\).\n\\[Y = \\sum_{i=1}^{n} Z_i^2 = \\sum_{i=1}^{n}\\left(\\frac{X_i - \\mu}{\\sigma}\\right)^2\\]\nis a Chi-square random variable with \\(n\\) degrees of freedom, i.e.: \\(Y \\sim \\chi^2_{(n)}\\)."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#chi-square-distribution-main-properties-1",
    "href": "slides/02_Sampling_Distributions.html#chi-square-distribution-main-properties-1",
    "title": "Sampling Distributions",
    "section": "Chi-Square Distribution: Main Properties (1)",
    "text": "Chi-Square Distribution: Main Properties (1)"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#chi-square-distribution-main-properties-2",
    "href": "slides/02_Sampling_Distributions.html#chi-square-distribution-main-properties-2",
    "title": "Sampling Distributions",
    "section": "Chi-Square Distribution: Main Properties (2)",
    "text": "Chi-Square Distribution: Main Properties (2)\n\nThe graphical shape of the distribution depends on its degrees of freedom: \\(n\\) (\\(n \\in \\mathbb{N}\\)).\nIt is a positive and non-symmetric function. The skewness decreases as the degrees of freedom increase.\nIf \\(X \\sim \\chi^2_{(n)}\\), then: \\(E(X) = n\\) and \\(Var(X) = 2n\\).\nIt is an additive distribution. The sum of \\(k\\) independent random variables, where \\(X_i \\sim \\chi^2_{(n_i)}\\), is still a random variable such that: \\[\\sum_{i=1}^k X_i \\sim \\chi^2_{\\left(n = \\sum_{i=1}^k n_i\\right)}\\]\n\n\n\nWith increasing degrees of freedom, the Chi-square distribution tends to the Normal distribution, by the C.L.T.:\n\\[\\chi^2_{(n)} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N(n,\\;\\sqrt{2n}) \\iff Z_n = \\frac{\\chi^2_{(n)} - n}{\\sqrt{2n}} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-v",
    "href": "slides/02_Sampling_Distributions.html#exercise-v",
    "title": "Sampling Distributions",
    "section": "Exercise V",
    "text": "Exercise V\nLet \\((Z_1, Z_2, Z_3, Z_4, Z_5)\\) be a random sample from a \\(N(0, 1)\\) population and \\(Q = \\sum_{i=1}^5 Z_i^2\\).\n\n[V][F] \\(\\quad P(1.610 &lt; Q &lt; 9.236) = 0.8\\)\n[V][F] \\(\\quad\\) The standard deviation of \\(Z_3^2 + Z_4^2\\) is equal to 4.\n\n\na)\n\\(Q \\sim \\chi^2_{(5)}\\)\n\\[P(1.610 &lt; Q &lt; 9.236) = P(Q &gt; 1.610) - P(Q &gt; 9.236) = 0.9 - 0.1 = 0.8\\]\n(direct observation from the \\(\\chi^2\\) table)\nThe statement is true.\n\n\nb)\n\\(Z_3^2 \\sim \\chi^2_{(1)}\\) ; \\(Z_4^2 \\sim \\chi^2_{(1)}\\) ; \\(Z_3^2 + Z_4^2 \\sim \\chi^2_{(2)}\\) (Additivity Theorem)\n\\[Var(Z_3^2 + Z_4^2) = 2 \\times 2 = 4 \\implies \\sigma(Z_3^2 + Z_4^2) = \\sqrt{4} = 2\\]\nThe statement is false."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#students-t-distribution",
    "href": "slides/02_Sampling_Distributions.html#students-t-distribution",
    "title": "Sampling Distributions",
    "section": "Student‚Äôs t Distribution",
    "text": "Student‚Äôs t Distribution\nLet \\(X \\sim Normal(0, 1)\\) and \\(Y \\sim \\chi^2_{(n)}\\) be independent random variables. Then the random variable:\n\\[T = \\frac{X}{\\sqrt{Y/n}} \\sim t_{(n)}\\]\nhas a Student‚Äôs \\(t\\) distribution with \\(n\\) degrees of freedom, i.e.: \\(T \\sim t_{(n)}\\).\n\nMain properties of the Student‚Äôs t distribution\n\nThe graphical shape depends on its degrees of freedom (\\(n\\)). As the d.f. increase, the quantiles of the Student‚Äôs \\(t\\) converge to the corresponding quantiles of the Standard Normal."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#students-t-distribution-properties",
    "href": "slides/02_Sampling_Distributions.html#students-t-distribution-properties",
    "title": "Sampling Distributions",
    "section": "Student‚Äôs t Distribution: Properties",
    "text": "Student‚Äôs t Distribution: Properties\n\n\nIt is a symmetric function around the mean.\nIf \\(X \\sim t_{(n)}\\), then: \\(E(X) = 0\\) and \\(Var(X) = \\dfrac{n}{n-2}\\), with \\(n &gt; 2\\)."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#students-t-distribution-clt-approximation",
    "href": "slides/02_Sampling_Distributions.html#students-t-distribution-clt-approximation",
    "title": "Sampling Distributions",
    "section": "Student‚Äôs t Distribution: CLT Approximation",
    "text": "Student‚Äôs t Distribution: CLT Approximation\n\nWith increasing degrees of freedom, the Student‚Äôs \\(t\\) distribution tends to the Normal distribution, by the C.L.T.:\n\\[X \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N\\left(0, \\;\\frac{n}{n-2}\\right) \\iff Z_n = \\frac{X}{\\sqrt{\\frac{n}{n-2}}} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#case-1-normal-population-sigma2-known",
    "href": "slides/02_Sampling_Distributions.html#case-1-normal-population-sigma2-known",
    "title": "Sampling Distributions",
    "section": "Case 1: Normal Population, \\(\\sigma^2\\) Known",
    "text": "Case 1: Normal Population, \\(\\sigma^2\\) Known\nPopulation: \\(X \\sim N(\\mu, \\sigma)\\), with \\(\\sigma^2\\) known.\nFrom a random sample \\(X_1, X_2, \\ldots, X_n\\), where \\(X_i \\sim N(\\mu, \\sigma)\\), we obtain the statistic sample mean:\n\\[\\bar{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\]\n\nThe sampling distribution of \\(\\bar{X}\\) is still Normal, by the additivity theorem of the Normal distribution:\n\\[\\bar{X} \\sim N\\left(\\mu, \\;\\frac{\\sigma}{\\sqrt{n}}\\right) \\iff Z = \\frac{\\bar{X} - \\mu}{\\dfrac{\\sigma}{\\sqrt{n}}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#case-2-normal-population-sigma2-unknown",
    "href": "slides/02_Sampling_Distributions.html#case-2-normal-population-sigma2-unknown",
    "title": "Sampling Distributions",
    "section": "Case 2: Normal Population, \\(\\sigma^2\\) Unknown",
    "text": "Case 2: Normal Population, \\(\\sigma^2\\) Unknown\nPopulation: \\(X \\sim N(\\mu, \\sigma)\\), with \\(\\sigma^2\\) unknown.\nStatistic: Mean of a random sample of size \\(n\\): \\(\\bar{X} = \\dfrac{1}{n}\\displaystyle\\sum_{i=1}^n X_i\\)\n\nSampling distribution of \\(\\bar{X}\\):\n\\[T = \\frac{\\bar{X} - \\mu}{\\dfrac{S}{\\sqrt{n-1}}} = \\frac{\\bar{X} - \\mu}{\\dfrac{S'}{\\sqrt{n}}} \\sim t_{(n-1)}\\]\n\n\n\nNote: It is rare that one needs to infer the mean value when the variance is known. Thus, it is necessary to estimate the parameter \\(\\sigma^2\\) using the sample variance \\(S^2\\) or the corrected sample variance \\(S'^2\\)."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#case-2-cont.-and-case-3",
    "href": "slides/02_Sampling_Distributions.html#case-2-cont.-and-case-3",
    "title": "Sampling Distributions",
    "section": "Case 2 (cont.) and Case 3",
    "text": "Case 2 (cont.) and Case 3\n\\[S^2 = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\bar{X})^2 \\qquad \\text{and} \\qquad S'^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\bar{X})^2 = \\frac{n}{n-1}S^2\\]\n\nApproximate sampling distribution of \\(\\bar{X}\\): for \\(n\\) sufficiently large (\\(n \\geq 30\\)), by the C.L.T.:\n\\[Z = \\frac{\\bar{X} - \\mu}{\\dfrac{S}{\\sqrt{n-1}}} = \\frac{\\bar{X} - \\mu}{\\dfrac{S'}{\\sqrt{n}}} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N(0, 1)\\]\n\n\nCase 3: Unknown population, with mean \\(\\mu\\) and known variance \\(\\sigma^2\\).\nStatistic: Mean of a random sample of size \\(n\\) (\\(n \\geq 30\\)): \\(\\bar{X} = \\dfrac{1}{n}\\displaystyle\\sum_{i=1}^n X_i\\)"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#case-3-cont.",
    "href": "slides/02_Sampling_Distributions.html#case-3-cont.",
    "title": "Sampling Distributions",
    "section": "Case 3 (cont.)",
    "text": "Case 3 (cont.)\nApproximate sampling distribution of \\(\\bar{X}\\): by the C.L.T.,\n\\[Z = \\frac{\\bar{X} - \\mu}{\\dfrac{\\sigma}{\\sqrt{n}}} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N(0, 1)\\]\n\nNote: If \\(\\sigma^2\\) is unknown, we have the approximate sampling distribution:\n\\[Z = \\frac{\\bar{X} - \\mu}{\\dfrac{S}{\\sqrt{n-1}}} = \\frac{\\bar{X} - \\mu}{\\dfrac{S'}{\\sqrt{n}}} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-vii-statement",
    "href": "slides/02_Sampling_Distributions.html#exercise-vii-statement",
    "title": "Sampling Distributions",
    "section": "Exercise VII (Statement)",
    "text": "Exercise VII (Statement)\nA certain company intends to validate the value (in euros) of its accounts receivable from clients, with an underlying distributional behavior of the account values being \\(Normal(\\mu = 385, \\sigma = ?)\\). A random sample of 25 client accounts (independent) was selected, yielding a corrected sample standard deviation of \\(s' = 122.6\\) euros.\n\n\nV F \\(\\quad\\) The sampling distribution of the mean value of client accounts is \\(t_{(25)}\\).\n\n\n\n\nV F \\(\\quad\\) The sample size to be collected (assuming the corrected sample standard deviation remains the same) so that the sample mean does not deviate from the population mean by more than 20 euros in 90% of cases is \\(n = 102\\). (Assume, for the calculations, that the value of \\(n\\) to be found will be greater than 30.)"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-vii-solution-a",
    "href": "slides/02_Sampling_Distributions.html#exercise-vii-solution-a",
    "title": "Sampling Distributions",
    "section": "Exercise VII ‚Äî Solution: a)",
    "text": "Exercise VII ‚Äî Solution: a)\nPopulation: \\(X \\sim Normal(\\mu = 385, \\sigma = ?)\\) (standard deviation unknown)\nSample: \\(n = 25 \\rightarrow s' = 122.6\\)\n\na)\nThe sampling distribution of the statistic sample mean (mean value of client accounts) is given by:\n\\[T = \\frac{\\bar{X} - \\mu}{\\dfrac{s'}{\\sqrt{n}}} \\sim t_{(n-1)} \\equiv \\frac{\\bar{X} - \\mu}{\\dfrac{s'}{\\sqrt{n}}} \\sim t_{(24)}\\]\nThe statement is false."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-vii-solution-b",
    "href": "slides/02_Sampling_Distributions.html#exercise-vii-solution-b",
    "title": "Sampling Distributions",
    "section": "Exercise VII ‚Äî Solution: b)",
    "text": "Exercise VII ‚Äî Solution: b)\nb)\nPopulation: \\(X \\sim Normal(\\mu = 385, \\sigma = ?)\\) (standard deviation unknown)\nSample: \\(n \\geq 30 \\rightarrow s' = 122.6\\)\n\n\\[\\bar{X} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N\\left(\\mu, \\;\\frac{s'}{\\sqrt{n}}\\right) \\equiv N\\left(385, \\;\\frac{122.6}{\\sqrt{n}}\\right)\\]\n\n\n\\[P(|\\bar{X} - \\mu| &lt; 20) = 0.9 \\iff P\\left(|Z| &lt; \\frac{20}{\\dfrac{122.6}{\\sqrt{n}}}\\right) = 0.9\\]\n\n\n\\[\\iff P\\left(|Z| &gt; \\frac{20}{\\dfrac{122.6}{\\sqrt{n}}}\\right) = 0.1\\]\n\n\n\\[\\iff \\frac{20}{\\dfrac{122.6}{\\sqrt{n}}} = 1.645 \\iff n \\geq 102\\]\nThe statement is true."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#sampling-distribution-of-s2",
    "href": "slides/02_Sampling_Distributions.html#sampling-distribution-of-s2",
    "title": "Sampling Distributions",
    "section": "Sampling Distribution of \\(S^2\\)",
    "text": "Sampling Distribution of \\(S^2\\)\nPopulation: \\(X \\sim N(\\mu, \\sigma)\\), with mean \\(\\mu\\) unknown and variance \\(\\sigma^2\\).\nSelected a random sample \\(X_1, X_2, \\ldots, X_n\\), where \\(X_i \\sim N(\\mu, \\sigma)\\), we obtain the statistic sample variance \\(S^2\\) or corrected sample variance \\(S'^2\\):\n\nSampling distribution of \\(S^2\\) or \\(S'^2\\):\n\\[Q = \\frac{(n-1)S'^2}{\\sigma^2} = \\frac{nS^2}{\\sigma^2} \\sim \\chi^2_{(n-1)}\\]"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-viii",
    "href": "slides/02_Sampling_Distributions.html#exercise-viii",
    "title": "Sampling Distributions",
    "section": "Exercise VIII",
    "text": "Exercise VIII\nA pharmaceutical company produces pills in which the variability of the quantity of the active substance from one pill to another should be very small. The standard deviation of the population is, supposedly, one milligram. Inspectors from the Ministry of Health selected a random sample of 16 pills. Assuming the population is Normal, the probability that the corrected sample variance is greater than \\(0.736\\;mg^2\\) is equal to:\n\n0.76 \\(\\qquad\\) b) 0.25 \\(\\qquad\\) c) 0.75 \\(\\qquad\\) d) none of the above\n\n\nPopulation: \\(X \\sim N(\\mu, \\sigma)\\)\nPivotal variable:\n\\[Q = \\frac{(n-1)S'^2}{\\sigma^2} = \\frac{(n-1)S'^2}{1} = (n-1)S'^2 \\sim \\chi^2_{(n-1)} \\equiv \\chi^2_{(15)}\\]\n\n\nWe want:\n\\[P(S'^2 &gt; 0.736) = P(Q &gt; 15 \\times 0.736) = P(Q &gt; 11.04) \\approx 0.75\\]\n(direct observation from the \\(\\chi^2\\) distribution table)\nThe correct answer is c)."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#case-1-total-of-successes",
    "href": "slides/02_Sampling_Distributions.html#case-1-total-of-successes",
    "title": "Sampling Distributions",
    "section": "Case 1: Total of Successes",
    "text": "Case 1: Total of Successes\nSelected a random sample \\(X_1, X_2, \\ldots, X_n\\) from a Bernoulli population, where \\(X_i \\sim Bernoulli(1, p)\\), and let \\(Y\\) be the number of elements that possess the characteristic of interest (successes) in the sample of size \\(n\\).\n\nCase 1:\nThe statistic \\(Y = \\sum_{i=1}^n X_i\\) (total or absolute frequency of successes) has the sampling distribution:\n\\[Y = \\sum_{i=1}^n X_i \\sim Binomial(n, p)\\]\n\n\nIf \\(n\\) is sufficiently large, the C.L.T. justifies the approximate sampling distribution:\n\\[Y \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N\\left(np, \\;\\sqrt{np(1-p)}\\right) \\iff Z_n = \\frac{Y - np}{\\sqrt{np(1-p)}} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#case-2-sample-proportion",
    "href": "slides/02_Sampling_Distributions.html#case-2-sample-proportion",
    "title": "Sampling Distributions",
    "section": "Case 2: Sample Proportion",
    "text": "Case 2: Sample Proportion\nCase 2:\nLet the statistic \\(\\bar{X} = \\dfrac{1}{n}\\displaystyle\\sum_{i=1}^n X_i = \\dfrac{Y}{n}\\), the proportion (relative frequency) of successes that, in a sample of size \\(n\\), possess a given attribute.\n\nIt is important to know the sampling distribution of this statistic \\(\\bar{X} = \\hat{p}\\).\n\n\n\\[E(\\bar{X}) = E\\left(\\frac{X_1 + X_2 + \\cdots + X_n}{n}\\right) = \\frac{1}{n}E(X_1 + X_2 + \\cdots + X_n) = \\frac{1}{n}np = p\\]\n\\[Var(\\bar{X}) = Var\\left(\\frac{X_1 + X_2 + \\cdots + X_n}{n}\\right) = \\frac{1}{n^2}Var(X_1 + \\cdots + X_n) = \\frac{1}{n^2}npq = \\frac{p(1-p)}{n}\\]"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#case-2-approximate-distribution-of-hatp",
    "href": "slides/02_Sampling_Distributions.html#case-2-approximate-distribution-of-hatp",
    "title": "Sampling Distributions",
    "section": "Case 2: Approximate Distribution of \\(\\hat{p}\\)",
    "text": "Case 2: Approximate Distribution of \\(\\hat{p}\\)\nWhen \\(n \\to \\infty\\), the C.L.T. yields the approximate sampling distribution:\n\\[\\bar{X} = \\hat{p} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N\\left(p, \\;\\sqrt{\\frac{p(1-p)}{n}}\\right) \\iff Z_n = \\frac{\\bar{X} - p}{\\sqrt{\\dfrac{p(1-p)}{n}}} \\;\\overset{\\text{CLT}}{\\dot\\sim}\\; N(0, 1)\\]"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-x-statement",
    "href": "slides/02_Sampling_Distributions.html#exercise-x-statement",
    "title": "Sampling Distributions",
    "section": "Exercise X (Statement)",
    "text": "Exercise X (Statement)\nIn a certain neighborhood, the proportion of inhabitants who consider the opening of a new supermarket useful is 0.8. The supermarket chain Gota Doce intends to open a new store in this neighborhood. Nevertheless, before making a decision, a study was conducted in which 400 inhabitants were randomly interviewed to assess their intention to use the services of the new supermarket.\n\n\nV F \\(\\quad\\) The approximate probability that, in the sample of interviewed inhabitants, at least 300 consider the opening of the Gota Doce supermarket useful is equal to 0.9948.\n\n\n\n\nV F \\(\\quad\\) The probability that the deviation between the sample proportion of inhabitants who stated they would actually use the services of the new supermarket, and the true proportion, is less than 0.02, is 0.8413."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-x-solution-a",
    "href": "slides/02_Sampling_Distributions.html#exercise-x-solution-a",
    "title": "Sampling Distributions",
    "section": "Exercise X ‚Äî Solution: a)",
    "text": "Exercise X ‚Äî Solution: a)\n\\(p = 0.8\\) (proportion of inhabitants who consider the opening of a new supermarket useful)\na) \\(X \\to\\) number of inhabitants, out of 400, who consider the opening of the new supermarket useful.\n\\[X \\sim Binomial(n = 400, p = 0.8)\\]\n\n\\[P(X \\geq 300) = P(X \\geq 299.5) \\approx P\\left(Z \\geq \\frac{299.5 - 400 \\times 0.8}{\\sqrt{400 \\times 0.8 \\times 0.2}}\\right)\\]\n\n\n\\[= P(Z \\geq -2.56) = P(Z \\leq 2.56) = \\Phi(2.56) = 0.9948\\]\n\n\nNote: 1) Continuity correction \\(\\quad\\) 2) CLT, approximation of the Binomial to the Normal distribution\n\\[Z = \\frac{X - np}{\\sqrt{np(1-p)}} \\;\\dot\\sim\\; N(0,1)\\]\nThe statement is true."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-x-solution-b",
    "href": "slides/02_Sampling_Distributions.html#exercise-x-solution-b",
    "title": "Sampling Distributions",
    "section": "Exercise X ‚Äî Solution: b)",
    "text": "Exercise X ‚Äî Solution: b)\nb) Of the interviews conducted, \\(X\\) inhabitants out of \\(n\\) expressed the opening of Gota Doce was useful, so the sample proportion is \\(\\frac{X}{n}\\).\n\n\\[P\\left(\\left|\\frac{X}{n} - p\\right| &lt; 0.02\\right) \\approx P\\left(|Z| &lt; \\frac{0.02}{\\sqrt{\\dfrac{0.8 \\times 0.2}{400}}}\\right) = P(|Z| &lt; 1)\\]\n\n\n\\[= P(-1 &lt; Z &lt; 1) = \\Phi(1) - \\Phi(-1) = \\Phi(1) - [1 - \\Phi(1)]\\]\n\n\n\\[= 2\\Phi(1) - 1 = 2 \\times 0.8413 - 1 = 0.6826\\]\n\n\nNote: Observe that \\(\\dfrac{X - np}{\\sqrt{np(1-p)}} = \\dfrac{\\frac{X}{n} - p}{\\sqrt{\\dfrac{p(1-p)}{n}}}\\) and that, by the CLT,\n\\[Z = \\frac{X - np}{\\sqrt{np(1-p)}} \\;\\dot\\sim\\; N(0,1) \\quad\\text{and, therefore, also}\\quad Z = \\frac{\\frac{X}{n} - p}{\\sqrt{\\dfrac{p(1-p)}{n}}} \\;\\dot\\sim\\; N(0,1)\\]\nThe statement is false."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#disclaimer",
    "href": "slides/02_Sampling_Distributions.html#disclaimer",
    "title": "Sampling Distributions",
    "section": "Disclaimer",
    "text": "Disclaimer\nThese slides are a free translation and adaptation from the slide deck for Distribui√ß√µes por Amostragem (Sampling Distributions) by Prof.¬†Sandra Gancho Cust√≥dio, Prof.¬†Teresa Ferreira, and Prof.¬†Sofia Ant√≥nio from the Lisbon Accounting and Business School ‚Äî Polytechnic University of Lisbon."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-ii",
    "href": "slides/02_Sampling_Distributions.html#exercise-ii",
    "title": "Sampling Distributions",
    "section": "Exercise II",
    "text": "Exercise II\nLet \\(X_1, X_2, \\ldots, X_n\\), \\(n \\in \\mathbb{N}\\), be a random sample from a population \\(X\\) (with unknown probabilistic behavior), with expected value \\(\\mu\\) and variance \\(\\sigma^2\\), both known.\n\nThe expected value of the statistics \\(\\displaystyle\\sum_{i=1}^{n} X_i\\) and \\(\\bar{X}\\) are, respectively, equal to:\na)\\(n\\mu\\) and \\(\\mu\\) \\(\\quad\\) b) \\(n\\mu\\) and \\(\\mu/n\\) \\(\\quad\\) c) \\(\\mu\\) and \\(\\mu/n\\) \\(\\quad\\) d) none of the above\n\n\nAnswer: (a)\n\nThe variance of the statistics \\(\\displaystyle\\sum_{i=1}^{n} X_i\\) and \\(\\bar{X}\\) are, respectively, equal to:\n\n\n\n\\(n\\sigma^2\\) and \\(\\sigma\\) \\(\\quad\\) b)\\(n\\sigma^2\\) and \\(\\sigma^2/n\\) \\(\\quad\\) c) \\(\\sigma^2\\) and \\(\\sigma^2/n\\) \\(\\quad\\) d) none of the above\n\n\n\nAnswer: (b)"
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#example",
    "href": "slides/02_Sampling_Distributions.html#example",
    "title": "Sampling Distributions",
    "section": "Example",
    "text": "Example\nExercise 4: Let \\(T\\) be a r.v distributed \\(t-Student\\) with 11 d.o.f., \\(T\\sim t(11)\\).\ni) [T][F] The value of \\(k\\) such that \\(P(T\\leq k)=0.75\\) is equal to \\(0.303\\). ii) [T][F] The value of the probability \\(P(|T|&gt;2.201)\\) is:\na) 0.025 b) 0.95 c) 0.05 d) None\n\n\n\\(P(T\\leq k)=0.75\\ \\Leftrightarrow\\ P(T&gt;k)=0.25\\ \\Leftrightarrow\\ k=0.697\\) directly from the table, and the the sentence is False.\n\\(P(|T|&gt;2.201)=2 P(T&gt;2.201) = 2\\times 0.025 = 0.05\\) by symmetry of the distribution and direct observation of the table. The correct answer is c."
  },
  {
    "objectID": "slides/02_Sampling_Distributions.html#exercise-vii",
    "href": "slides/02_Sampling_Distributions.html#exercise-vii",
    "title": "Sampling Distributions",
    "section": "Exercise VII",
    "text": "Exercise VII\nA certain company intends to validate the value (in euros) of its accounts receivable from clients, with an underlying distributional behavior of the account values being \\(Normal(\\mu = 385, \\sigma = ?)\\). A random sample of 25 client accounts (independent) was selected, yielding a corrected sample standard deviation of \\(s' = 122.6\\) euros.\n\n\nV F \\(\\quad\\) The sampling distribution of the mean value of client accounts is \\(t_{(25)}\\).\n\n\n\n\nV F \\(\\quad\\) The sample size to be collected (assuming the corrected sample standard deviation remains the same) so that the sample mean does not deviate from the population mean by more than 20 euros in 90% of cases is \\(n = 102\\). (Assume, for the calculations, that the value of \\(n\\) to be found will be greater than 30.)"
  },
  {
    "objectID": "slides/point_estimation.html",
    "href": "slides/point_estimation.html",
    "title": "Point Estimation",
    "section": "",
    "text": "Point Estimation ‚Äî Overview\nProperties of Estimators\nSolved Exercises"
  },
  {
    "objectID": "slides/point_estimation.html#outline",
    "href": "slides/point_estimation.html#outline",
    "title": "Point Estimation",
    "section": "",
    "text": "Point Estimation ‚Äî Overview\nProperties of Estimators\nSolved Exercises"
  },
  {
    "objectID": "slides/point_estimation.html#statistical-inference",
    "href": "slides/point_estimation.html#statistical-inference",
    "title": "Point Estimation",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n\nPopulation\n\nParameters (unknown)\n\n\\(\\downarrow\\) Random Sample \\(\\uparrow\\)\nSample\n\nStatistics (known)\n\n\n\n\n\n\n\n\nNoteStatistical Inference\n\n\n\nUsing sample information to draw conclusions about population parameters."
  },
  {
    "objectID": "slides/point_estimation.html#point-estimation-cont.",
    "href": "slides/point_estimation.html#point-estimation-cont.",
    "title": "Point Estimation",
    "section": "Point Estimation (cont.)",
    "text": "Point Estimation (cont.)\nLet \\(X \\sim N(\\mu, \\sigma)\\), with \\(\\mu\\) and \\(\\sigma\\) unknown.\nLet \\(X_1, X_2, \\ldots, X_n\\) be a random sample from population \\(X\\).\nGoal: How can we estimate \\(\\mu\\), the population mean, from the sample \\(X_1, X_2, \\ldots, X_n\\)?\n. . .\n\n\n\n\n\n\nThe objective of point estimation is to use all available information from the sample in order to select a single value that is the most plausible for the (unknown) parameter to be estimated."
  },
  {
    "objectID": "slides/point_estimation.html#point-estimation-cont.-1",
    "href": "slides/point_estimation.html#point-estimation-cont.-1",
    "title": "Point Estimation",
    "section": "Point Estimation (cont.)",
    "text": "Point Estimation (cont.)\nLet \\(X\\) be a random variable whose probabilistic behaviour is known and characterised by a parameter \\(\\theta\\), which is unknown.\nIf \\(X_1, X_2, \\ldots, X_n\\) is a random sample of size \\(n\\) from that population, a point estimator of \\(\\theta\\), denoted \\(\\hat{\\theta}\\), is any statistic \\(T(X_1, X_2, \\ldots, X_n)\\) that takes values only in \\(\\Theta\\) (the set of possible values for \\(\\theta\\)).\nOnce a particular sample \\(x_1, x_2, \\ldots, x_n\\) is observed, we obtain a point estimate for \\(\\theta\\): \\(T(x_1, x_2, \\ldots, x_n)\\)."
  },
  {
    "objectID": "slides/point_estimation.html#point-estimation-cont.-2",
    "href": "slides/point_estimation.html#point-estimation-cont.-2",
    "title": "Point Estimation",
    "section": "Point Estimation (cont.)",
    "text": "Point Estimation (cont.)\nThere are specific methods that allow us to choose the estimator for each population parameter to be estimated, such as the maximum likelihood method, the method of moments, and others.\n. . .\nBecause there may be several estimators for the same population parameter, we consider some properties that estimators should ideally possess, which serve as guidance on how to choose the ‚Äúbest‚Äù one."
  },
  {
    "objectID": "slides/point_estimation.html#unbiased-estimator",
    "href": "slides/point_estimation.html#unbiased-estimator",
    "title": "Point Estimation",
    "section": "Unbiased Estimator",
    "text": "Unbiased Estimator\n\n\n\n\n\n\nNoteDefinition\n\n\n\nAn estimator \\(\\hat{\\theta}\\) is said to be unbiased (or centred) for the parameter \\(\\theta\\) if and only if: \\[E(\\hat{\\theta}) = \\theta\\]\nAn estimator \\(\\hat{\\theta}\\) is said to be biased for the parameter \\(\\theta\\) if and only if: \\[E(\\hat{\\theta}) \\neq \\theta\\]\nThe bias of an estimator \\(\\hat{\\theta}\\) is measured by: \\[\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\]"
  },
  {
    "objectID": "slides/point_estimation.html#unbiased-estimator-examples",
    "href": "slides/point_estimation.html#unbiased-estimator-examples",
    "title": "Point Estimation",
    "section": "Unbiased Estimator ‚Äî Examples",
    "text": "Unbiased Estimator ‚Äî Examples\nConsider a population \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nGiven a random sample \\(X_1, X_2, \\ldots, X_n\\) from \\(X\\), the following hold for any distributional behaviour of \\(X\\) (provided \\(E(X)\\) and \\(V(X)\\) exist):\n. . .\n\\[E(\\bar{X}) = \\mu \\quad \\Rightarrow \\quad \\bar{X} \\text{ is an unbiased estimator for } \\mu\\]\n. . .\n\\[E(S'^2) = \\sigma^2 \\quad \\Rightarrow \\quad S'^2 \\text{ is an unbiased estimator for } \\sigma^2\\]\n. . .\n\\[E(S^2) = \\frac{n-1}{n}\\sigma^2 \\neq \\sigma^2 \\quad \\Rightarrow \\quad S^2 \\text{ is NOT an unbiased estimator for } \\sigma^2\\]"
  },
  {
    "objectID": "slides/point_estimation.html#relative-efficiency-of-unbiased-estimators",
    "href": "slides/point_estimation.html#relative-efficiency-of-unbiased-estimators",
    "title": "Point Estimation",
    "section": "Relative Efficiency of Unbiased Estimators",
    "text": "Relative Efficiency of Unbiased Estimators\n\n\n\n\n\n\nNoteDefinition\n\n\n\nLet \\(\\hat{\\theta}\\) and \\(\\tilde{\\theta}\\) be two unbiased estimators of the same parameter \\(\\theta\\).\nThe estimator \\(\\hat{\\theta}\\) is said to be more efficient than \\(\\tilde{\\theta}\\) if:\n\\[\\text{Var}(\\hat{\\theta}) &lt; \\text{Var}(\\tilde{\\theta}) \\quad \\Longleftrightarrow \\quad \\frac{\\text{Var}(\\hat{\\theta})}{\\text{Var}(\\tilde{\\theta})} &lt; 1\\]"
  },
  {
    "objectID": "slides/point_estimation.html#simply-consistent-estimator",
    "href": "slides/point_estimation.html#simply-consistent-estimator",
    "title": "Point Estimation",
    "section": "(Simply) Consistent Estimator",
    "text": "(Simply) Consistent Estimator\n\n\n\n\n\n\nNoteSufficient Condition for Consistency\n\n\n\nA sufficient condition for an estimator \\(\\hat{\\theta}\\) to be consistent for the parameter \\(\\theta\\) is:\n\\[\\text{i)} \\quad \\lim_{n \\to +\\infty} E(\\hat{\\theta}) = \\theta\\]\n\\[\\text{ii)} \\quad \\lim_{n \\to +\\infty} \\text{Var}(\\hat{\\theta}) = 0\\]"
  },
  {
    "objectID": "slides/point_estimation.html#exercise-1",
    "href": "slides/point_estimation.html#exercise-1",
    "title": "Point Estimation",
    "section": "Exercise 1",
    "text": "Exercise 1\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\), \\(n \\in \\mathbb{N}\\), drawn from a population \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), both finite.\nConsider the estimator \\(T_1\\) for \\(\\mu\\):\n\\[T_1 = X_1 + \\frac{1}{n-1}\\sum_{i=2}^{n} X_i\\]\na) True or False: This estimator is biased for \\(\\mu\\) and its bias equals \\(\\mu\\)."
  },
  {
    "objectID": "slides/point_estimation.html#exercise-1-solution-a",
    "href": "slides/point_estimation.html#exercise-1-solution-a",
    "title": "Point Estimation",
    "section": "Exercise 1 ‚Äî Solution (a)",
    "text": "Exercise 1 ‚Äî Solution (a)\n\\[E(T_1) = E\\!\\left(X_1 + \\frac{1}{n-1}\\sum_{i=2}^{n}X_i\\right) = E(X_1) + \\frac{1}{n-1}\\sum_{i=2}^{n}E(X_i)\\]\n. . .\n\\[= \\mu + \\frac{1}{n-1}\\sum_{i=2}^{n}\\mu = \\mu + \\frac{1}{n-1}(n-1)\\mu = 2\\mu \\neq \\mu\\]\n. . .\nTherefore, \\(T_1\\) is a biased estimator for \\(\\mu\\).\n\\[\\text{Bias}(T_1) = E(T_1) - \\mu = 2\\mu - \\mu = \\mu\\]\nThe statement is true."
  },
  {
    "objectID": "slides/point_estimation.html#exercise-1-part-b",
    "href": "slides/point_estimation.html#exercise-1-part-b",
    "title": "Point Estimation",
    "section": "Exercise 1 ‚Äî Part (b)",
    "text": "Exercise 1 ‚Äî Part (b)\nNow consider a second estimator \\(T_2\\) for \\(\\mu\\):\n\\[T_2 = \\frac{1}{2}T_1\\]\nTrue or False: \\(T_2\\) is a consistent estimator for \\(\\mu\\).\n. . .\nFor \\(T_2\\) to be consistent for \\(\\mu\\), we need to verify:\n\\[\\text{i)} \\quad \\lim_{n \\to +\\infty} E(T_2) = \\mu \\qquad \\text{ii)} \\quad \\lim_{n \\to +\\infty} \\text{Var}(T_2) = 0\\]"
  },
  {
    "objectID": "slides/point_estimation.html#exercise-1-solution-b-part-i",
    "href": "slides/point_estimation.html#exercise-1-solution-b-part-i",
    "title": "Point Estimation",
    "section": "Exercise 1 ‚Äî Solution (b), Part i",
    "text": "Exercise 1 ‚Äî Solution (b), Part i\n\\[\\lim_{n \\to +\\infty} E(T_2) = \\lim_{n \\to +\\infty} E\\!\\left(\\frac{1}{2}T_1\\right) = \\lim_{n \\to +\\infty} \\frac{1}{2}E(T_1) = \\lim_{n \\to +\\infty} \\frac{1}{2} \\times 2\\mu = \\mu \\checkmark\\]\nCondition i) is satisfied."
  },
  {
    "objectID": "slides/point_estimation.html#exercise-1-solution-b-part-ii",
    "href": "slides/point_estimation.html#exercise-1-solution-b-part-ii",
    "title": "Point Estimation",
    "section": "Exercise 1 ‚Äî Solution (b), Part ii",
    "text": "Exercise 1 ‚Äî Solution (b), Part ii\nWe need \\(\\lim_{n \\to +\\infty}\\text{Var}(T_2)\\). Let us first compute \\(\\text{Var}(T_1)\\):\n\\[\\text{Var}(T_1) = \\text{Var}\\!\\left(X_1 + \\frac{1}{n-1}\\sum_{i=2}^{n}X_i\\right) = \\sigma^2 + \\frac{1}{(n-1)^2}\\sum_{i=2}^{n}\\sigma^2 = \\sigma^2 + \\frac{n-1}{(n-1)^2}\\sigma^2 = \\sigma^2 + \\frac{\\sigma^2}{n-1}\\]\n. . .\nThen:\n\\[\\lim_{n \\to +\\infty}\\text{Var}(T_2) = \\lim_{n \\to +\\infty}\\frac{1}{4}\\text{Var}(T_1) = \\frac{1}{4}\\lim_{n \\to +\\infty}\\left(\\sigma^2 + \\frac{\\sigma^2}{n-1}\\right) = \\frac{1}{4}\\sigma^2 \\neq 0\\]\n. . .\nCondition ii) is not satisfied. Therefore \\(T_2\\) is not a consistent estimator for \\(\\mu\\).\nThe statement is false."
  },
  {
    "objectID": "slides/point_estimation.html#exercise-2",
    "href": "slides/point_estimation.html#exercise-2",
    "title": "Point Estimation",
    "section": "Exercise 2",
    "text": "Exercise 2\nConsider a population \\(X\\) whose distribution depends on a parameter \\(\\theta \\in \\mathbb{R}\\), with unknown value. It is known that:\n\\[E(X) = \\theta - 2 \\qquad \\text{and} \\qquad V(X) = 1\\]\nFrom a random sample \\(X_1, X_2, \\ldots, X_n\\), \\(n \\geq 2\\), two estimators \\(\\theta^*\\) and \\(\\hat{\\theta}\\) were obtained, with the following known properties:\n\\[\\theta^* = \\bar{X} + 2, \\qquad E(\\hat{\\theta}) = \\theta, \\qquad V(\\hat{\\theta}) = \\frac{2}{n}\\]"
  },
  {
    "objectID": "slides/point_estimation.html#exercise-2-part-i",
    "href": "slides/point_estimation.html#exercise-2-part-i",
    "title": "Point Estimation",
    "section": "Exercise 2 ‚Äî Part i",
    "text": "Exercise 2 ‚Äî Part i\ni) Regarding estimators \\(\\theta^*\\) and \\(\\hat{\\theta}\\):\n\n\n\na) Only \\(\\theta^*\\) is unbiased\nb) Neither estimator is unbiased\n\n\nc) Both are unbiased\nd) Only \\(\\hat{\\theta}\\) is unbiased\n\n\n\n. . .\nSolution:\nPopulation \\(X\\): \\(\\mu = E(X) = \\theta - 2\\); \\(\\sigma^2 = V(X) = 1\\)\n\\(E(\\hat{\\theta}) = \\theta\\) ‚Äî given in the problem statement, so \\(\\hat{\\theta}\\) is unbiased.\n\\[E(\\theta^*) = E(\\bar{X} + 2) = E(\\bar{X}) + 2 = \\mu + 2 = (\\theta - 2) + 2 = \\theta\\]\nSince \\(E(\\theta^*) = E(\\hat{\\theta}) = \\theta\\), both estimators are unbiased. Answer: (c)."
  },
  {
    "objectID": "slides/point_estimation.html#exercise-2-part-ii",
    "href": "slides/point_estimation.html#exercise-2-part-ii",
    "title": "Point Estimation",
    "section": "Exercise 2 ‚Äî Part ii",
    "text": "Exercise 2 ‚Äî Part ii\nii) True or False: \\(\\theta^*\\) is a more efficient estimator than \\(\\hat{\\theta}\\).\n. . .\nSolution:\nBoth estimators are unbiased, so the more efficient one has the smaller variance:\n\\[V(\\theta^*) = V(\\bar{X} + 2) = V(\\bar{X}) = \\frac{\\sigma^2}{n} = \\frac{1}{n}\\]\n. . .\n\\[V(\\theta^*) = \\frac{1}{n} &lt; \\frac{2}{n} = V(\\hat{\\theta})\\]\nTherefore, \\(\\theta^*\\) is more efficient than \\(\\hat{\\theta}\\).\nThe statement is true."
  },
  {
    "objectID": "slides/01_CLT_slides.html",
    "href": "slides/01_CLT_slides.html",
    "title": "Central Limit Theorem",
    "section": "",
    "text": "Additivity (Normal Distribution):\nIf:\n\n\\(X_i \\sim Normal(\\mu,\\sigma)\\), \\(i = 1,\\ldots,n\\) ‚Äî independent and identically distributed random variables\n\\(T = X_1 + \\ldots + X_n\\) and \\(\\bar{X} = \\dfrac{T}{n}\\)\n\nthen:\n\\[T \\sim Normal\\!\\left(\\mu_T = n \\times \\mu,\\sigma_T = \\sqrt{n} \\times \\sigma\\right)\\]\n\\[\\bar{X} \\sim Normal\\!\\left(\\mu_{\\bar{X}} = \\mu,\\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\\right)\\]"
  },
  {
    "objectID": "slides/03_point_estimation.html#outline",
    "href": "slides/03_point_estimation.html#outline",
    "title": "Point Estimation",
    "section": "Outline",
    "text": "Outline\n\nPoint Estimation ‚Äî Overview\nProperties of Estimators\nSolved Exercises"
  },
  {
    "objectID": "slides/03_point_estimation.html#statistical-inference",
    "href": "slides/03_point_estimation.html#statistical-inference",
    "title": "Point Estimation",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\n  \n    \n      \n    \n    \n      \n    \n  \n\n  \n  \n  Population\n  Parameters (unknown)\n\n  \n  \n  Sample\n  Statistics (known)\n\n  \n  \n  \n  Random Sampling\n\n  \n  \n  \n  Statistical Inference"
  },
  {
    "objectID": "slides/03_point_estimation.html#point-estimation-cont.",
    "href": "slides/03_point_estimation.html#point-estimation-cont.",
    "title": "Point Estimation",
    "section": "Point Estimation (cont.)",
    "text": "Point Estimation (cont.)\nLet \\(X \\sim N(\\mu, \\sigma)\\), with \\(\\mu\\) and \\(\\sigma\\) unknown.\nLet \\(X_1, X_2, \\ldots, X_n\\) be a random sample from population \\(X\\).\nGoal: How can we estimate \\(\\mu\\), the population mean, from the sample \\(X_1, X_2, \\ldots, X_n\\)?\n\n\n\n\nThe objective of point estimation is to use all available information from the sample in order to select a single value that is the most plausible for the (unknown) parameter to be estimated."
  },
  {
    "objectID": "slides/03_point_estimation.html#point-estimation-cont.-1",
    "href": "slides/03_point_estimation.html#point-estimation-cont.-1",
    "title": "Point Estimation",
    "section": "Point Estimation (cont.)",
    "text": "Point Estimation (cont.)\nLet \\(X\\) be a random variable whose probabilistic behaviour is known and characterised by a parameter \\(\\theta\\), which is unknown.\nIf \\(X_1, X_2, \\ldots, X_n\\) is a random sample of size \\(n\\) from that population, a point estimator of \\(\\theta\\), denoted \\(\\hat{\\theta}\\), is any statistic \\(T(X_1, X_2, \\ldots, X_n)\\) that takes values only in \\(\\Theta\\) (the set of possible values for \\(\\theta\\)).\nOnce a particular sample \\(x_1, x_2, \\ldots, x_n\\) is observed, we obtain a point estimate for \\(\\theta\\): \\(T(x_1, x_2, \\ldots, x_n)\\)."
  },
  {
    "objectID": "slides/03_point_estimation.html#point-estimation-cont.-2",
    "href": "slides/03_point_estimation.html#point-estimation-cont.-2",
    "title": "Point Estimation",
    "section": "Point Estimation (cont.)",
    "text": "Point Estimation (cont.)\nThere are specific methods that allow us to choose the estimator for each population parameter to be estimated, such as the maximum likelihood method, the method of moments, and others.\n\nBecause there may be several estimators for the same population parameter, we consider some properties that estimators should ideally possess, which serve as guidance on how to choose the ‚Äúbest‚Äù one."
  },
  {
    "objectID": "slides/03_point_estimation.html#unbiased-estimator",
    "href": "slides/03_point_estimation.html#unbiased-estimator",
    "title": "Point Estimation",
    "section": "Unbiased Estimator",
    "text": "Unbiased Estimator\n\n\n\nDefinition\n\n\nAn estimator \\(\\hat{\\theta}\\) is said to be unbiased (or centred) for the parameter \\(\\theta\\) if and only if: \\[E(\\hat{\\theta}) = \\theta\\]\nAn estimator \\(\\hat{\\theta}\\) is said to be biased for the parameter \\(\\theta\\) if and only if: \\[E(\\hat{\\theta}) \\neq \\theta\\]\nThe bias of an estimator \\(\\hat{\\theta}\\) is measured by: \\[\\text{Bias}(\\hat{\\theta}) = E(\\hat{\\theta}) - \\theta\\]"
  },
  {
    "objectID": "slides/03_point_estimation.html#unbiased-estimator-examples",
    "href": "slides/03_point_estimation.html#unbiased-estimator-examples",
    "title": "Point Estimation",
    "section": "Unbiased Estimator ‚Äî Examples",
    "text": "Unbiased Estimator ‚Äî Examples\nConsider a population \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\).\nGiven a random sample \\(X_1, X_2, \\ldots, X_n\\) from \\(X\\), the following hold for any distributional behaviour of \\(X\\) (provided \\(E(X)\\) and \\(V(X)\\) exist):\n\n\\[E(\\bar{X}) = \\mu \\quad \\Rightarrow \\quad \\bar{X} \\text{ is an unbiased estimator for } \\mu\\]\n\n\n\\[E(S'^2) = \\sigma^2 \\quad \\Rightarrow \\quad S'^2 \\text{ is an unbiased estimator for } \\sigma^2\\]\n\n\n\\[E(S^2) = \\frac{n-1}{n}\\sigma^2 \\neq \\sigma^2 \\quad \\Rightarrow \\quad S^2 \\text{ is NOT an unbiased estimator for } \\sigma^2\\]"
  },
  {
    "objectID": "slides/03_point_estimation.html#relative-efficiency-of-unbiased-estimators",
    "href": "slides/03_point_estimation.html#relative-efficiency-of-unbiased-estimators",
    "title": "Point Estimation",
    "section": "Relative Efficiency of Unbiased Estimators",
    "text": "Relative Efficiency of Unbiased Estimators\n\n\n\nDefinition\n\n\nLet \\(\\hat{\\theta}\\) and \\(\\tilde{\\theta}\\) be two unbiased estimators of the same parameter \\(\\theta\\).\nThe estimator \\(\\hat{\\theta}\\) is said to be more efficient than \\(\\tilde{\\theta}\\) if:\n\\[\\text{Var}(\\hat{\\theta}) &lt; \\text{Var}(\\tilde{\\theta}) \\quad \\Longleftrightarrow \\quad \\frac{\\text{Var}(\\hat{\\theta})}{\\text{Var}(\\tilde{\\theta})} &lt; 1\\]"
  },
  {
    "objectID": "slides/03_point_estimation.html#simply-consistent-estimator",
    "href": "slides/03_point_estimation.html#simply-consistent-estimator",
    "title": "Point Estimation",
    "section": "(Simply) Consistent Estimator",
    "text": "(Simply) Consistent Estimator\n\n\n\nSufficient Condition for Consistency\n\n\nA sufficient condition for an estimator \\(\\hat{\\theta}\\) to be consistent for the parameter \\(\\theta\\) is:\n\\[\\text{i)} \\quad \\lim_{n \\to +\\infty} E(\\hat{\\theta}) = \\theta\\]\n\\[\\text{ii)} \\quad \\lim_{n \\to +\\infty} \\text{Var}(\\hat{\\theta}) = 0\\]"
  },
  {
    "objectID": "slides/03_point_estimation.html#exercise-1",
    "href": "slides/03_point_estimation.html#exercise-1",
    "title": "Point Estimation",
    "section": "Exercise 1",
    "text": "Exercise 1\nConsider a sample \\(X_1, X_2, \\ldots, X_n\\), \\(n \\in \\mathbb{N}\\), drawn from a population \\(X\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\), both finite.\nConsider the estimator \\(T_1\\) for \\(\\mu\\):\n\\[T_1 = X_1 + \\frac{1}{n-1}\\sum_{i=2}^{n} X_i\\]\na) True or False: This estimator is biased for \\(\\mu\\) and its bias equals \\(\\mu\\)."
  },
  {
    "objectID": "slides/03_point_estimation.html#exercise-1-solution-a",
    "href": "slides/03_point_estimation.html#exercise-1-solution-a",
    "title": "Point Estimation",
    "section": "Exercise 1 ‚Äî Solution (a)",
    "text": "Exercise 1 ‚Äî Solution (a)\n\\[E(T_1) = E\\!\\left(X_1 + \\frac{1}{n-1}\\sum_{i=2}^{n}X_i\\right) = E(X_1) + \\frac{1}{n-1}\\sum_{i=2}^{n}E(X_i)\\]\n\n\\[= \\mu + \\frac{1}{n-1}\\sum_{i=2}^{n}\\mu = \\mu + \\frac{1}{n-1}(n-1)\\mu = 2\\mu \\neq \\mu\\]\n\n\nTherefore, \\(T_1\\) is a biased estimator for \\(\\mu\\).\n\\[\\text{Bias}(T_1) = E(T_1) - \\mu = 2\\mu - \\mu = \\mu\\]\nThe statement is true."
  },
  {
    "objectID": "slides/03_point_estimation.html#exercise-1-part-b",
    "href": "slides/03_point_estimation.html#exercise-1-part-b",
    "title": "Point Estimation",
    "section": "Exercise 1 ‚Äî Part (b)",
    "text": "Exercise 1 ‚Äî Part (b)\nNow consider a second estimator \\(T_2\\) for \\(\\mu\\):\n\\[T_2 = \\frac{1}{2}T_1\\]\nTrue or False: \\(T_2\\) is a consistent estimator for \\(\\mu\\).\n\nFor \\(T_2\\) to be consistent for \\(\\mu\\), we need to verify:\n\\[\\text{i)} \\quad \\lim_{n \\to +\\infty} E(T_2) = \\mu \\qquad \\text{ii)} \\quad \\lim_{n \\to +\\infty} \\text{Var}(T_2) = 0\\]"
  },
  {
    "objectID": "slides/03_point_estimation.html#exercise-1-solution-b-part-i",
    "href": "slides/03_point_estimation.html#exercise-1-solution-b-part-i",
    "title": "Point Estimation",
    "section": "Exercise 1 ‚Äî Solution (b), Part i",
    "text": "Exercise 1 ‚Äî Solution (b), Part i\n\\[\\lim_{n \\to +\\infty} E(T_2) = \\lim_{n \\to +\\infty} E\\!\\left(\\frac{1}{2}T_1\\right) = \\lim_{n \\to +\\infty} \\frac{1}{2}E(T_1) = \\lim_{n \\to +\\infty} \\frac{1}{2} \\times 2\\mu = \\mu \\checkmark\\]\nCondition i) is satisfied."
  },
  {
    "objectID": "slides/03_point_estimation.html#exercise-1-solution-b-part-ii",
    "href": "slides/03_point_estimation.html#exercise-1-solution-b-part-ii",
    "title": "Point Estimation",
    "section": "Exercise 1 ‚Äî Solution (b), Part ii",
    "text": "Exercise 1 ‚Äî Solution (b), Part ii\nWe need \\(\\lim_{n \\to +\\infty}\\text{Var}(T_2)\\). Let us first compute \\(\\text{Var}(T_1)\\):\n\\[\\text{Var}(T_1) = \\text{Var}\\!\\left(X_1 + \\frac{1}{n-1}\\sum_{i=2}^{n}X_i\\right) = \\sigma^2 + \\frac{1}{(n-1)^2}\\sum_{i=2}^{n}\\sigma^2 = \\sigma^2 + \\frac{n-1}{(n-1)^2}\\sigma^2 = \\sigma^2 + \\frac{\\sigma^2}{n-1}\\]\n\nThen:\n\\[\\lim_{n \\to +\\infty}\\text{Var}(T_2) = \\lim_{n \\to +\\infty}\\frac{1}{4}\\text{Var}(T_1) = \\frac{1}{4}\\lim_{n \\to +\\infty}\\left(\\sigma^2 + \\frac{\\sigma^2}{n-1}\\right) = \\frac{1}{4}\\sigma^2 \\neq 0\\]\n\n\nCondition ii) is not satisfied. Therefore \\(T_2\\) is not a consistent estimator for \\(\\mu\\).\nThe statement is false."
  },
  {
    "objectID": "slides/03_point_estimation.html#exercise-2",
    "href": "slides/03_point_estimation.html#exercise-2",
    "title": "Point Estimation",
    "section": "Exercise 2",
    "text": "Exercise 2\nConsider a population \\(X\\) whose distribution depends on a parameter \\(\\theta \\in \\mathbb{R}\\), with unknown value. It is known that:\n\\[E(X) = \\theta - 2 \\qquad \\text{and} \\qquad V(X) = 1\\]\nFrom a random sample \\(X_1, X_2, \\ldots, X_n\\), \\(n \\geq 2\\), two estimators \\(\\theta^*\\) and \\(\\hat{\\theta}\\) were obtained, with the following known properties:\n\\[\\theta^* = \\bar{X} + 2, \\qquad E(\\hat{\\theta}) = \\theta, \\qquad V(\\hat{\\theta}) = \\frac{2}{n}\\]"
  },
  {
    "objectID": "slides/03_point_estimation.html#exercise-2-part-i",
    "href": "slides/03_point_estimation.html#exercise-2-part-i",
    "title": "Point Estimation",
    "section": "Exercise 2 ‚Äî Part i",
    "text": "Exercise 2 ‚Äî Part i\ni) Regarding estimators \\(\\theta^*\\) and \\(\\hat{\\theta}\\):\n\n\n\na) Only \\(\\theta^*\\) is unbiased\nb) Neither estimator is unbiased\n\n\nc) Both are unbiased\nd) Only \\(\\hat{\\theta}\\) is unbiased\n\n\n\n\nSolution:\nPopulation \\(X\\): \\(\\mu = E(X) = \\theta - 2\\); \\(\\sigma^2 = V(X) = 1\\)\n\\(E(\\hat{\\theta}) = \\theta\\) ‚Äî given in the problem statement, so \\(\\hat{\\theta}\\) is unbiased.\n\\[E(\\theta^*) = E(\\bar{X} + 2) = E(\\bar{X}) + 2 = \\mu + 2 = (\\theta - 2) + 2 = \\theta\\]\nSince \\(E(\\theta^*) = E(\\hat{\\theta}) = \\theta\\), both estimators are unbiased. Answer: (c)."
  },
  {
    "objectID": "slides/03_point_estimation.html#exercise-2-part-ii",
    "href": "slides/03_point_estimation.html#exercise-2-part-ii",
    "title": "Point Estimation",
    "section": "Exercise 2 ‚Äî Part ii",
    "text": "Exercise 2 ‚Äî Part ii\nii) True or False: \\(\\theta^*\\) is a more efficient estimator than \\(\\hat{\\theta}\\).\n\nSolution:\nBoth estimators are unbiased, so the more efficient one has the smaller variance:\n\\[V(\\theta^*) = V(\\bar{X} + 2) = V(\\bar{X}) = \\frac{\\sigma^2}{n} = \\frac{1}{n}\\]\n\n\n\\[V(\\theta^*) = \\frac{1}{n} &lt; \\frac{2}{n} = V(\\hat{\\theta})\\]\nTherefore, \\(\\theta^*\\) is more efficient than \\(\\hat{\\theta}\\).\nThe statement is true."
  }
]